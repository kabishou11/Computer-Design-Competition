# 第一章：自然语言处理发展史

## 1.1 早期探索（1950-1980）

### 1.1.1 图灵测试

1950年，Alan Turing发表《Computing Machinery and Intelligence》，提出"模仿游戏"（图灵测试）：

> 如果机器能够与人类进行对话，而人类无法区分对方是人还是机器，则认为机器具有智能。

**深远影响**：
- 奠定了AI的哲学基础
- 推动了NLP的发展方向

### 1.1.2 早期规则系统

**1950-1980年代**的NLP主要基于规则：

1. **句法分析**（Chomsky句法理论）
2. **机器翻译**（冷战时期，美国政府大量投入）
3. **对话系统**（ELIZA，第一个聊天机器人）

**规则系统的局限**：
- 规则难以覆盖所有情况
- 手工编写规则成本高
- 泛化能力差

### 1.1.3 统计学习方法兴起

**1980年代**开始，统计方法逐渐取代规则：

1. **隐马尔可夫模型（HMM）**：序列标注
2. **条件随机场（CRF）**：结构化预测
3. **统计机器翻译（SMT）**：基于短语的翻译

**核心思想**：从数据中自动学习规律

## 1.2 深度学习革命（2006-2013）

### 1.2.1 深度学习的复兴

2006年，Hinton提出**深度信念网络**（DBN），开启了深度学习时代。

**关键技术突破**：
- 逐层预训练
- ReLU激活函数
- Dropout正则化

### 1.2.2 神经网络语言模型（2003）

Bengio等提出**神经网络语言模型**（NNLM）：

$$P(w_i | w_1, ..., w_{i-1}) = \text{softmax}(Wx + b)$$

**创新点**：
- 词向量作为副产品
- 端到端学习
- 首次展示深度学习的潜力

**问题**：训练速度慢

### 1.2.3 Word2Vec的突破（2013）

Mikolov的Word2Vec彻底改变NLP：

**成就**：
- 训练速度极快
- 词向量质量高
- 开源、易用

**历史意义**：
- 词向量成为NLP标配
- 开启了预训练时代

## 1.3 序列模型时代（2014-2017）

### 1.3.1 注意力机制诞生（2014）

Bahdanau等提出**神经机器翻译的注意力机制**：

> 不是将整个句子压缩成一个向量，而是动态地"关注"源句子的不同部分

**创新**：
- 软寻址机制
- 解决长序列问题
- 启发了后续所有工作

### 1.3.2 Seq2Seq架构（2014）

Sutskever等提出**序列到序列模型**：

$$\text{Encoder: } h_t = f(x_t, h_{t-1})$$

$$\text{Decoder: } y_t = g(h_t, y_{t-1})$$

**应用**：
- 机器翻译
- 文本摘要
- 对话生成

### 1.3.3 Transformer诞生（2017）

Vaswani等发表《Attention Is All You Need》：

**核心突破**：
- 完全基于注意力
- 并行计算高效
- 可扩展性强

**历史意义**：
- 开启了Transformer时代
- 成为NLP的主流架构

## 1.4 预训练时代（2018-2020）

### 1.4.1 BERT：双向预训练

2018年，Google发表BERT：

**创新**：
- 双向Transformer
- 掩码语言建模
- 下一句预测

**影响**：
- GLUE基准刷爆
- 预训练-微调范式确立

### 1.4.2 GPT：自回归预训练

OpenAI的GPT系列：

| 版本 | 年份 | 参数量 | 特点 |
|-----|------|-------|------|
| GPT-1 | 2018 | 110M | 验证预训练范式 |
| GPT-2 | 2019 | 1.5B | 展示零样本能力 |
| GPT-3 | 2020 | 175B | 涌现能力、少样本学习 |

### 1.4.3 多模态探索

- **ViT**（2020）：将Transformer应用于图像
- **CLIP**（2021）：图文对比学习
- **DALL-E**：文本到图像生成

## 1.5 大语言模型时代（2020-至今）

### 1.5.1 规模效应

**Scaling Laws**（Kaplan et al., 2021）：

$$L(N) = A N^{-\alpha} + L(N \to \infty)$$

**关键发现**：
- 损失随参数量幂律下降
- 存在"涌现能力"临界点

### 1.5.2 涌现能力（Emergent Abilities）

Wei et al., 2022 定义：

> 某些能力在小模型中不存在，当模型规模超过临界点后突然出现

**典型涌现能力**：
- 算术运算
- 逻辑推理
- 代码生成
- 多步推理

### 1.5.3 ChatGPT时刻（2022）

**ChatGPT** 的发布引发了AI热潮：

1. **产品形态**：对话式交互
2. **能力泛化**：多领域通用
3. **安全性**：RLHF对齐

### 1.5.4 开源社区崛起

2023年，开源模型爆发：

| 模型 | 机构 | 特点 |
|-----|------|------|
| LLaMA | Meta | 首个高质量开源 |
| Mistral | Mistral AI | 高效架构 |
| Qwen | 阿里 | 中文优化 |
| Yi | 零一万物 | 中英双语 |

## 1.6 NLP与AI的关系

### 1.6.1 NLP是AI的皇冠

NLP被认为是AI最困难的任务之一：

1. **语言复杂性**：语法、语义、语用
2. **歧义性**：一词多义、指代消解
3. **知识依赖**：常识、世界知识
4. **生成能力**：连贯、流畅、有意义

### 1.6.2 大模型如何改变NLP

**传统NLP**：
- 每个任务独立建模
- 需要大量标注数据
- 泛化能力有限

**大模型NLP**：
- 一个模型处理所有任务
- 零样本/少样本学习
- 跨任务泛化

## 1.7 本章小结

| 时期 | 关键技术 | 代表工作 | 核心范式 |
|-----|---------|---------|---------|
| 1950-1980 | 规则系统 | ELIZA | 符号主义 |
| 1980-2010 | 统计学习 | HMM, CRF | 统计方法 |
| 2013-2017 | 词向量、RNN | Word2Vec | 深度学习 |
| 2018-2020 | 预训练Transformer | BERT, GPT | 预训练-微调 |
| 2020-至今 | 大模型、涌现 | GPT-4 | 提示学习 |

**发展规律**：
1. 从规则到统计
2. 从浅层到深层
3. 从任务特定到通用
4. 从有监督到自监督

## 参考文献

1. Turing, A. M. (1950). Computing Machinery and Intelligence.
2. Bengio, Y., et al. (2003). A Neural Probabilistic Language Model.
3. Bahdanau, D., et al. (2014). Neural Machine Translation by Jointly Learning to Align and Translate.
4. Vaswani, A., et al. (2017). Attention Is All You Need.
5. Devlin, J., et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
6. Wei, J., et al. (2022). Emergent Abilities of Large Language Models.
