# 第四章：混合检索技术

## 4.1 混合检索概述

### 4.1.1 为什么需要混合检索

单一检索方式存在局限：

| 检索方式 | 优点 | 缺点 |
|---------|------|------|
| **向量检索** | 语义匹配强 | 精确关键词匹配弱 |
| **关键词检索** | 精确匹配强 | 无法处理同义词 |

混合检索结合两者优势：

```
┌─────────────────────────────────────────────────────────────────┐
│                      混合检索架构                                │
│                                                                 │
│   用户查询                                                        │
│       │                                                         │
│       ▼                                                         │
│   ┌─────────┐                                                   │
│   │ 查询处理  │  ← 重写、扩展、分词                               │
│   └────┬────┘                                                   │
│        │                                                        │
│        ▼                                                        │
│   ┌─────────┐      ┌─────────┐                                │
│   │ 语义检索  │      │ 关键词检索 │                                │
│   │(向量相似) │      │(BM25等)   │                                │
│   └────┬─────┘      └────┬─────┘                                │
│        │                 │                                       │
│        └────────┬────────┘                                      │
│                 ▼                                               │
│          ┌────────────┐                                         │
│          │  结果融合   │  ← RRF, 加权平均                         │
│          └─────┬──────┘                                         │
│                ▼                                                 │
│          ┌────────────┐                                         │
│          │ 重排序(Rerank)│  ← Cross-Encoder                      │
│          └─────┬──────┘                                         │
│                ▼                                                 │
│          ┌────────────┐                                         │
│          │  返回结果   │                                         │
│          └────────────┘                                         │
└─────────────────────────────────────────────────────────────────┘
```

## 4.2 关键词检索（BM25）

### 4.2.1 BM25原理

BM25是一种基于概率的关键词检索算法：

```
BM25(Q, D) = Σ IDF(qi) × (f(qi, D) × (k1 + 1)) / (f(qi, D) + k1 × (1 - b + b × |D|/avgdl))

其中：
- f(qi, D): qi在文档D中的词频
- |D|: 文档长度
- avgdl: 平均文档长度
- k1: 词频饱和参数 (通常1.2-2.0)
- b: 长度归一化参数 (通常0.75)
- IDF(qi): 逆文档频率
```

### 4.2.2 BM25实现

```python
import math
from collections import Counter
import numpy as np

class BM25:
    """BM25关键词检索"""

    def __init__(self, documents, k1=1.5, b=0.75):
        self.documents = documents
        self.k1 = k1
        self.b = b

        # 计算文档长度和平均长度
        self.doc_lengths = [len(doc.split()) for doc in documents]
        self.avg_doc_length = np.mean(self.doc_lengths)

        # 计算词频
        self.doc_freqs = []
        self.idf = {}
        self._build_index()

    def _build_index(self):
        """构建索引"""
        for doc in self.documents:
            words = doc.lower().split()
            freq = Counter(words)
            self.doc_freqs.append(freq)

        # 计算IDF
        num_docs = len(self.documents)
        for freq in self.doc_freqs:
            for word in freq.keys():
                doc_count = sum(1 for f in self.doc_freqs if word in f)
                self.idf[word] = math.log((num_docs - doc_count + 0.5) / (doc_count + 0.5) + 1)

    def score(self, query, doc_idx):
        """计算单个文档的BM25分数"""
        doc = self.documents[doc_idx]
        words = query.lower().split()
        doc_freq = self.doc_freqs[doc_idx]

        score = 0
        for word in words:
            if word not in self.idf:
                continue

            tf = doc_freq.get(word, 0)
            idf = self.idf[word]

            numerator = tf * (self.k1 + 1)
            denominator = tf + self.k1 * (1 - self.b + self.b * len(words) / self.avg_doc_length)

            score += idf * numerator / denominator

        return score

    def search(self, query, top_k=5):
        """检索"""
        scores = [self.score(query, i) for i in range(len(self.documents))]
        top_indices = np.argsort(scores)[::-1][:top_k]

        return [(idx, scores[idx]) for idx in top_indices]


# 使用示例
documents = [
    "人工智能是计算机科学的重要分支",
    "机器学习是实现人工智能的关键技术",
    "深度学习是机器学习的重要方法",
    "神经网络是深度学习的基础"
]

bm25 = BM25(documents)

results = bm25.search("深度学习", top_k=3)
for idx, score in results:
    print(f"文档{idx}: {documents[idx]}, 分数: {score:.4f}")
```

### 4.2.3 使用rank-bm25库

```python
# pip install rank-bm25

from rank_bm25 import BM25Okapi, BM25Plus
import re

class BM25Retriever:
    """基于rank-bm25的检索器"""

    def __init__(self, documents):
        # 分词函数
        def tokenize(text):
            # 简单的中文分词（按字符或使用jieba）
            return text.lower().split()

        self.bm25 = BM25Okapi([tokenize(doc) for doc in documents])
        self.documents = documents

    def search(self, query, top_k=5):
        """检索"""
        tokenized_query = query.lower().split()
        scores = self.bm25.get_scores(tokenized_query)

        top_indices = np.argsort(scores)[::-1][:top_k]

        return [
            {"id": int(idx), "text": self.documents[idx], "score": float(scores[idx])}
            for idx in top_indices
        ]

    def search_with_scores(self, query, top_k=5):
        """带详细分数的检索"""
        tokenized_query = query.lower().split()
        return self.bm25.get_top_n(tokenized_query, self.documents, n=top_k)
```

## 4.3 重排序（Rerank）

### 4.3.1 为什么需要重排序

- **向量检索**速度快但精度有限
- **Cross-Encoder**精度高但速度慢
- 两阶段策略：先用向量检索召回候选，再用Cross-Encoder精排

### 4.3.2 Rerank模型

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

class Reranker:
    """Cross-Encoder重排序模型"""

    def __init__(self, model_name="BAAI/bge-reranker-base"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.model.eval()

    def compute_score(self, query, document):
        """计算query和document的相关性分数"""
        inputs = self.tokenizer(
            query,
            document,
            truncation=True,
            max_length=512,
            return_tensors="pt"
        )

        with torch.no_grad():
            outputs = self.model(**inputs)
            score = torch.sigmoid(outputs.logits).item()

        return score

    def rerank(self, query, documents, top_k=3):
        """对文档列表重排序"""
        # 计算所有文档的分数
        scores = []
        for doc in documents:
            score = self.compute_score(query, doc)
            scores.append((doc, score))

        # 按分数排序
        sorted_docs = sorted(scores, key=lambda x: x[1], reverse=True)

        return sorted_docs[:top_k]


# 使用示例
reranker = Reranker("BAAI/bge-reranker-base")

query = "什么是深度学习？"
documents = [
    "深度学习是机器学习的一个分支",
    "人工智能的发展历史",
    "神经网络是深度学习的基础"
]

reranked = reranker.rerank(query, documents, top_k=3)
for doc, score in reranked:
    print(f"分数: {score:.4f}, 内容: {doc}")
```

### 4.3.3 轻量级重排序

```python
class LightReranker:
    """轻量级重排序（基于特征工程）"""

    def __init__(self):
        pass

    def rerank(self, query, documents, top_k=3):
        """基于多特征重排序"""
        scores = []
        for doc in documents:
            # 词重叠分数
            overlap = self._word_overlap(query, doc)
            # 位置分数（查询词在文档中的位置）
            position = self._position_score(query, doc)
            # 长度分数（文档长度适中最好）
            length = self._length_score(doc)

            # 综合分数
            total_score = overlap * 0.6 + position * 0.3 + length * 0.1
            scores.append((doc, total_score))

        return sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]

    def _word_overlap(self, query, doc):
        """词重叠分数"""
        query_words = set(query.lower().split())
        doc_words = set(doc.lower().split())
        overlap = len(query_words & doc_words)
        return overlap / len(query_words) if query_words else 0

    def _position_score(self, query, doc):
        """位置分数"""
        query_words = query.lower().split()
        doc_lower = doc.lower()

        positions = []
        for word in query_words:
            pos = doc_lower.find(word)
            if pos >= 0:
                # 越靠前分数越高
                positions.append(1 - pos / len(doc_lower))
            else:
                positions.append(0)

        return np.mean(positions)

    def _length_score(self, doc):
        """长度分数"""
        length = len(doc.split())
        if length < 50:
            return 0.5
        elif length > 1000:
            return 0.3
        else:
            return 1.0
```

## 4.4 结果融合

### 4.4.1 RRF（Reciprocal Rank Fusion）

```python
def rrf_fusion(results_list, k=60):
    """
    RRF融合多个检索结果

    RRF(d) = Σ (1 / (k + rank_i(d)))
    """
    fused_scores = {}

    for results in results_list:
        for rank, (doc_id, score) in enumerate(results):
            rrf_score = 1 / (k + rank + 1)
            fused_scores[doc_id] = fused_scores.get(doc_id, 0) + rrf_score

    # 按分数排序
    sorted_docs = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)

    return sorted_docs
```

### 4.4.2 加权融合

```python
def weighted_fusion(results_list, weights):
    """
    加权融合多个检索结果

    Score(d) = Σ (weight_i × score_i(d))
    """
    # 收集所有文档ID
    all_doc_ids = set()
    for results in results_list:
        all_doc_ids.update(doc_id for doc_id, _ in results)

    # 归一化分数
    normalized_results = []
    for results, weight in zip(results_list, weights):
        if not results:
            continue

        scores = [score for _, score in results]
        max_score = max(scores)
        min_score = min(scores)

        if max_score == min_score:
            normalized = [1.0] * len(scores)
        else:
            normalized = [(s - min_score) / (max_score - min_score) for s in scores]

        normalized_results.append([(doc_id, norm_score * weight)
                                   for (doc_id, _), norm_score in zip(results, normalized)])

    # 合并分数
    fused_scores = {}
    for results in normalized_results:
        for doc_id, score in results:
            fused_scores[doc_id] = fused_scores.get(doc_id, 0) + score

    return sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
```

### 4.4.3 混合检索完整实现

```python
class HybridRetriever:
    """混合检索器"""

    def __init__(self, documents, embedding_model=None):
        self.documents = documents

        # 关键词检索
        self.bm25 = BM25Retriever(documents)

        # 向量检索
        if embedding_model:
            self.vector_retriever = VectorRetriever(documents, embedding_model)

    def hybrid_search(
        self,
        query,
        vector_weight=0.5,
        keyword_weight=0.5,
        top_k_vector=50,
        top_k_bm25=50,
        top_k_final=10
    ):
        """混合检索"""
        # 1. 并行执行两种检索
        bm25_results = self.bm25.search(query, top_k=top_k_bm25)

        if hasattr(self, 'vector_retriever'):
            vector_results = self.vector_retriever.search(query, top_k=top_k_vector)
        else:
            vector_results = []

        # 2. 收集所有候选文档
        all_candidates = set()
        for results in [bm25_results, vector_results]:
            for doc_id, _ in results:
                all_candidates.add(doc_id)

        # 3. 归一化分数
        def normalize(results):
            if not results:
                return {}
            scores = [s for _, s in results]
            max_s, min_s = max(scores), min(scores)
            if max_s == min_s:
                return {doc_id: 1.0 for doc_id, _ in results}
            return {doc_id: (s - min_s) / (max_s - min_s) for doc_id, s in results}

        bm25_norm = normalize(bm25_results)
        vector_norm = normalize(vector_results)

        # 4. 加权融合
        fused_scores = {}
        for doc_id in all_candidates:
            bm25_s = bm25_norm.get(doc_id, 0)
            vector_s = vector_norm.get(doc_id, 0)
            fused_scores[doc_id] = bm25_s * keyword_weight + vector_s * vector_weight

        # 5. 返回top_k
        sorted_results = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)

        return [
            {"id": doc_id, "text": self.documents[doc_id], "score": score}
            for doc_id, score in sorted_results[:top_k_final]
        ]


class VectorRetriever:
    """向量检索器"""

    def __init__(self, documents, embedding_model):
        self.documents = documents
        self.embedder = embedding_model
        self.embeddings = self.embedder.encode(documents)

    def search(self, query, top_k=10):
        """向量检索"""
        query_emb = self.embedder.encode_single(query)
        scores = np.dot(query_emb, self.embeddings.T)

        top_indices = np.argsort(scores)[::-1][:top_k]

        return [(int(idx), float(scores[idx])) for idx in top_indices]
```

## 4.5 查询优化

### 4.5.1 查询扩展

```python
class QueryExpander:
    """查询扩展器"""

    def __init__(self, llm_model):
        self.llm = llm_model

    def expand_query(self, query):
        """生成查询的多个变体"""
        prompt = f"""请为以下查询生成3个不同的表达方式，保持语义相似：

原始查询：{query}

要求：
1. 使用不同的同义词
2. 调整问法使其更精确
3. 保持原意不变

返回格式：
1. 变体1
2. 变体2
3. 变体3"""

        response = self.llm.generate(prompt)
        variations = [v.strip() for v in response.split('\n') if v.strip()]

        return [query] + variations

    def expand_with_synonyms(self, query):
        """使用同义词扩展"""
        # 可以接入知识图谱或同义词库
        synonyms = {
            "人工智能": ["AI", "机器智能", "计算机智能"],
            "深度学习": ["深度神经网络", "DL", "深层学习"],
            "机器学习": ["ML", "自动学习", "统计学习"]
        }

        expanded = [query]
        for word, syns in synonyms.items():
            if word in query:
                expanded.extend(syns)

        return expanded
```

### 4.5.2 查询重写

```python
class QueryRewriter:
    """查询重写器"""

    def __init__(self, llm_model):
        self.llm = llm_model

    def rewrite_query(self, query):
        """将口语化查询转换为精确查询"""
        prompt = f"""请将以下口语化查询重写为更精确、更适合检索的查询：

口语化查询：{query}

重写后的查询："""

        rewritten = self.llm.generate(prompt)
        return rewritten.strip()

    def decompose_query(self, query):
        """将复杂查询分解为多个简单查询"""
        prompt = f"""将以下复杂查询分解为多个简单查询，每个查询关注一个方面：

复杂查询：{query}

分解后的查询：
1.
2.
3."""

        response = self.llm.generate(prompt)
        lines = [l.strip() for l in response.split('\n') if l.strip() and l[0].isdigit()]
        return [l.split('.', 1)[1].strip() for l in lines]
```

### 4.5.3 假设文档嵌入（HyDE）

```python
class HyDERetriever:
    """基于HyDE的检索器"""

    def __init__(self, llm, retriever):
        self.llm = llm
        self.retriever = retriever

    def retrieve(self, query, top_k=5):
        """HyDE检索"""
        # 1. 生成假设文档
        hypothetical_doc = self._generate_hypothetical_doc(query)

        # 2. 用假设文档检索
        results = self.retriever.search(hypothetical_doc, top_k=top_k)

        return results

    def _generate_hypothetical_doc(self, query):
        """生成假设文档"""
        prompt = f"""假设你是领域专家，请写出关于"{query}"的详细回答：

回答："""

        doc = self.llm.generate(prompt)
        return doc
```

## 4.6 本章小结

本章介绍了混合检索的核心技术：
- BM25关键词检索
- Cross-Encoder重排序
- RRF和加权融合方法
- 查询扩展和重写
- HyDE假设文档嵌入

## 练习题

1. 实现BM25检索器并与向量检索对比
2. 使用重排序模型提升检索精度
3. 设计多查询检索策略
4. 实现完整的混合检索系统

## 参考资料

- [BM25算法详解](https://en.wikipedia.org/wiki/Okapi_BM25)
- [RRF融合算法](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf/)
- [HyDE论文](https://arxiv.org/abs/2212.10496) - 假设文档嵌入
- [BGE Reranker](https://github.com/FlagOpen/FlagEmbedding) - 中文重排序模型
