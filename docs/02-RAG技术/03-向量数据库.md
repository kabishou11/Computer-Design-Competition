# 第三章：向量数据库

## 3.1 向量数据库概述

向量数据库是专门用于存储和检索高维向量数据的数据库系统，是RAG系统的核心组件。

### 3.1.1 为什么需要向量数据库

| 需求 | 传统数据库 | 向量数据库 |
|-----|----------|----------|
| **检索方式** | 精确匹配 | 相似度检索 |
| **查询效率** | O(n) | O(log n) |
| **数据类型** | 结构化 | 高维向量 |
| **应用场景** | 事务处理 | AI检索 |

### 3.1.2 主流向量数据库对比

| 数据库 | 类型 | 优点 | 缺点 | 适用场景 |
|-------|------|------|------|---------|
| **Milvus** | 专用向量库 | 功能全、性能高 | 部署复杂 | 大规模生产 |
| **FAISS** | 向量索引库 | 速度快、无需服务 | 单机 | 实验/小规模 |
| **Chroma** | 轻量向量库 | 简单易用 | 功能有限 | 快速原型 |
| **Weaviate** | 向量搜索引擎 | 特性丰富 | 资源占用高 | 知识图谱 |
| **Qdrant** | 专用向量库 | 性能好、Go实现 | 较新 | 实时检索 |
| **Pinecone** | 托管向量库 | 完全托管 | 商业付费 | 企业级 |

## 3.2 Milvus实战

### 3.2.1 Milvus架构

```
┌─────────────────────────────────────────────────────────────┐
│                      Milvus 集群架构                          │
│                                                              │
│   ┌─────────────┐  ┌─────────────┐  ┌─────────────┐          │
│   │   Proxy     │  │   Proxy     │  │   Proxy     │  ← 接入层 │
│   └──────┬──────┘  └──────┬──────┘  └──────┬──────┘          │
│          │                │                │                  │
│          └────────────────┼────────────────┘                  │
│                           │                                 │
│                    ┌──────┴──────┐                         │
│                    │   Coordinator │  ← 协调层              │
│                    │   Services    │                        │
│                    └──────┬──────┘                         │
│                           │                                 │
│    ┌──────────────────────┼──────────────────────┐          │
│    │                      │                      │          │
│ ┌──┴──┐             ┌────┴────┐            ┌────┴────┐    │
│ │Data │             │   Index │            │  Query  │    │
│ │Node │             │  Nodes  │            │  Nodes  │    │
│ └─────┘             └─────────┘            └─────────┘    │
│   存储                  索引                   查询        │
└─────────────────────────────────────────────────────────────┘
```

### 3.2.2 Milvus安装与连接

```python
# 安装
# pip install pymilvus

from pymilvus import connections, utility

# 方式1：连接本地Milvus
connections.connect("default", host="localhost", port="19530")

# 方式2：连接远程Milvus
connections.connect(
    "default",
    host="milvus.example.com",
    port="19530",
    user="username",
    password="password"
)

# 检查连接
print("Milvus连接成功！")

# 列出所有集合
print("集合列表:", utility.list_collections())
```

### 3.2.3 创建Collection

```python
from pymilvus import CollectionSchema, FieldSchema, DataType
import numpy as np

# 定义Collection Schema
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
    FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=1024),
    FieldSchema(name="metadata", dtype=DataType.VARCHAR, max_length=65535, nullable=True)
]

schema = CollectionSchema(fields=fields, description="RAG知识库")

# 创建Collection
collection_name = "knowledge_base"
if utility.has_collection(collection_name):
    utility.drop_collection(collection_name)

collection = Collection(name=collection_name, schema=schema)

# 创建索引
index_params = {
    "metric_type": "COSINE",
    "index_type": "IVF_FLAT",
    "params": {"nlist": 1024}
}

collection.create_index(field_name="vector", index_params=index_params)

# 加载Collection
collection.load()
print(f"Collection '{collection_name}' 创建成功！")
```

### 3.2.4 数据操作

```python
from pymilvus import Collection

collection = Collection("knowledge_base")

# 1. 插入数据
def insert_documents(documents, embeddings, metadata=None):
    """批量插入文档"""
    entities = []

    for i, (doc, emb) in enumerate(zip(documents, embeddings)):
        meta = metadata[i] if metadata else "{}"
        entities.append([doc, emb.tolist(), meta])

    # 注意：id由Milvus自动生成，不在entities中
    result = collection.insert(entities)
    return result.insert_cnt

# 插入示例数据
documents = [
    "人工智能是计算机科学的一个重要分支",
    "机器学习是实现人工智能的一种方法",
    "深度学习是机器学习的一个重要分支"
]

embeddings = np.random.randn(3, 1024).astype(np.float32)
metadatas = ['{"source": "AI_intro", "page": 1}', '{}', '{}']

count = insert_documents(documents, embeddings, metadatas)
print(f"成功插入 {count} 条文档")

# 2. 搜索
def search_documents(query_embedding, top_k=5):
    """搜索相似文档"""
    search_params = {
        "metric_type": "COSINE",
        "params": {"nprobe": 10}
    }

    results = collection.search(
        data=[query_embedding.tolist()],
        anns_field="vector",
        param=search_params,
        limit=top_k,
        output_fields=["text", "metadata"]
    )

    return results[0]

# 搜索示例
query_vec = np.random.randn(1024).astype(np.float32)
results = search_documents(query_vec, top_k=3)

for hit in results:
    print(f"ID: {hit.id}, Distance: {hit.distance:.4f}")
    print(f"Text: {hit.entity.get('text')}")
    print("---")

# 3. 删除
def delete_documents(ids):
    """根据ID删除文档"""
    expr = f"id in {ids}"
    collection.delete(expr)
    print(f"删除 {len(ids)} 条文档")

# 4. 查询
def get_documents_by_metadata(source):
    """根据元数据查询"""
    results = collection.query(
        expr=f'metadata like "%{source}%"',
        output_fields=["id", "text", "metadata"]
    )
    return results
```

### 3.2.5 混合检索

```python
from pymilvus import Hits, Hit

class HybridRetriever:
    """混合检索器（向量+关键词）"""

    def __init__(self, collection_name):
        self.collection = Collection(collection_name)
        self.bm25_model = self._init_bm25()

    def _init_bm25(self):
        """初始化BM25模型用于关键词检索"""
        from rank_bm25 import BM25Okapi
        import re

        # 加载所有文本构建索引
        all_data = self.collection.query(
            expr="id >= 0",
            output_fields=["id", "text"]
        )

        self.doc_map = {doc["id"]: doc["text"] for doc in all_data}
        documents = list(self.doc_map.values())

        # 分词
        tokenized_docs = [doc.lower().split() for doc in documents]
        self.bm25 = BM25Okapi(tokenized_docs)
        self.doc_ids = list(self.doc_map.keys())

        return self.bm25

    def hybrid_search(self, query, vector_weight=0.7, top_k=10):
        """混合检索"""
        # 向量检索
        vector_results = self.vector_search(query, top_k=top_k * 2)

        # 关键词检索
        bm25_results = self.bm25_search(query, top_k=top_k * 2)

        # 融合结果
        fused_scores = self.fuse_scores(
            vector_results,
            bm25_results,
            vector_weight
        )

        # 返回top_k
        sorted_results = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_results[:top_k]

    def vector_search(self, query, top_k):
        """向量检索"""
        query_vec = self.encode_query(query)

        search_params = {
            "metric_type": "COSINE",
            "params": {"nprobe": 10}
        }

        results = self.collection.search(
            data=[query_vec.tolist()],
            anns_field="vector",
            param=search_params,
            limit=top_k,
            output_fields=["id"]
        )

        return {hit.id: hit.distance for hit in results[0]}

    def bm25_search(self, query, top_k):
        """BM25关键词检索"""
        import re

        tokenized_query = query.lower().split()
        scores = self.bm25.get_scores(tokenized_query)

        # 归一化
        scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)

        top_indices = np.argsort(scores)[::-1][:top_k]
        return {self.doc_ids[i]: scores[i] for i in top_indices}

    def fuse_scores(self, vector_scores, bm25_scores, vector_weight):
        """融合分数"""
        all_ids = set(vector_scores.keys()) | set(bm25_scores.keys())

        fused = {}
        for doc_id in all_ids:
            v_score = vector_scores.get(doc_id, 0)
            b_score = bm25_scores.get(doc_id, 0)
            fused[doc_id] = vector_weight * v_score + (1 - vector_weight) * b_score

        return fused

    def encode_query(self, query):
        """编码查询"""
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer("BAAI/bge-large-zh")
        return model.encode([query], normalize_embeddings=True)[0]
```

## 3.3 FAISS实战

### 3.3.1 FAISS索引类型

```python
import faiss
import numpy as np

# 索引类型对比
INDEX_TYPES = {
    "Flat": {
        "class": faiss.IndexFlatIP,
        "description": "精确检索，适合小规模数据",
        "memory": "高",
        "speed": "慢"
    },
    "IVF_FLAT": {
        "class": lambda d: faiss.IndexIVFFlat(faiss.IndexFlatIP(d), d, 1024),
        "description": "倒排索引，适合中等规模",
        "memory": "中",
        "speed": "中"
    },
    "IVF_PQ": {
        "class": lambda d: faiss.IndexIVFPQ(faiss.IndexFlatIP(d), d, 1024, 8, 8),
        "description": "乘积量化，大规模数据",
        "memory": "低",
        "speed": "快"
    },
    "HNSW": {
        "class": faiss.IndexHNSWFlat,
        "description": "图索引，高精度",
        "memory": "高",
        "speed": "快"
    }
}
```

### 3.3.2 FAISS索引操作

```python
class FAISSVectorStore:
    """基于FAISS的向量存储"""

    def __init__(self, dimension=1024, index_type="HNSW"):
        self.dimension = dimension
        self.index_type = index_type
        self.documents = []
        self.index = self._create_index()

    def _create_index(self):
        """创建索引"""
        if self.index_type == "HNSW":
            index = faiss.IndexHNSWFlat(self.dimension, 32)
            index.hnsw.efSearch = 128
            index.hnsw.efConstruction = 200
        elif self.index_type == "IVF_FLAT":
            quantizer = faiss.IndexFlatIP(self.dimension)
            index = faiss.IndexIVFFlat(quantizer, self.dimension, 1024)
        elif self.index_type == "Flat":
            index = faiss.IndexFlatIP(self.dimension)
        else:
            raise ValueError(f"Unknown index type: {self.index_type}")

        return index

    def add_documents(self, texts, embeddings):
        """添加文档"""
        # 归一化向量（用于余弦相似度）
        faiss.normalize_L2(embeddings)

        self.index.add(embeddings)
        self.documents.extend(texts)

        return len(texts)

    def search(self, query_embedding, top_k=5):
        """搜索"""
        faiss.normalize_L2(query_embedding.reshape(1, -1))

        if self.index_type == "HNSW":
            self.index.hnsw.efSearch = max(top_k * 2, 128)

        scores, indices = self.index.search(query_embedding.reshape(1, -1), top_k)

        results = []
        for i, (idx, score) in enumerate(zip(indices[0], scores[0])):
            if idx < len(self.documents):
                results.append({
                    "id": int(idx),
                    "text": self.documents[idx],
                    "score": float(score)
                })

        return results

    def save(self, index_path, docs_path):
        """保存"""
        faiss.write_index(self.index, index_path)

        import json
        with open(docs_path, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False)

    def load(self, index_path, docs_path):
        """加载"""
        self.index = faiss.read_index(index_path)

        import json
        with open(docs_path, 'r', encoding='utf-8') as f:
            self.documents = json.load(f)


# 使用示例
vector_store = FAISSVectorStore(dimension=1024, index_type="HNSW")

# 添加文档
texts = ["人工智能是...", "机器学习是...", "深度学习是..."]
embeddings = np.random.randn(3, 1024).astype(np.float32)

vector_store.add_documents(texts, embeddings)

# 搜索
query = "什么是深度学习？"
query_emb = np.random.randn(1024).astype(np.float32)

results = vector_store.search(query_emb, top_k=3)
for r in results:
    print(f"Score: {r['score']:.4f}, Text: {r['text']}")
```

## 3.4 Chroma实战

```python
# 安装
# pip install chromadb

import chromadb.utils import embedding_functions
from chromadb

class ChromaVectorStore:
    """基于Chroma的向量存储"""

    def __init__(self, collection_name="knowledge_base"):
        self.client = chromadb.PersistentClient("./chroma_db")

        # 使用中文嵌入模型
        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="BAAI/bge-large-zh"
        )

        # 获取或创建Collection
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            embedding_function=embedding_function
        )

    def add_documents(self, ids, documents, metadatas=None):
        """添加文档"""
        self.collection.add(
            documents=documents,
            ids=ids,
            metadatas=metadatas
        )

    def search(self, query, n_results=5):
        """搜索"""
        results = self.collection.query(
            query_texts=[query],
            n_results=n_results,
            include=["documents", "metadatas", "distances"]
        )

        return results

    def delete(self, ids):
        """删除"""
        self.collection.delete(ids=ids)


# 使用示例
vector_store = ChromaVectorStore()

# 添加文档
vector_store.add_documents(
    ids=["doc1", "doc2", "doc3"],
    documents=[
        "人工智能的发展历史",
        "机器学习算法介绍",
        "深度学习与神经网络"
    ],
    metadatas=[
        {"source": "wiki", "category": "AI"},
        {"source": "textbook", "category": "ML"},
        {"source": "paper", "category": "DL"}
    ]
)

# 搜索
results = vector_store.search("什么是深度学习？", n_results=2)
print(results)
```

## 3.5 向量数据库选择建议

```python
def recommend_vector_db(scenario):
    """根据场景推荐向量数据库"""
    recommendations = {
        "learning": {
            "db": "Chroma",
            "reason": "简单易用，无需额外服务，适合学习"
        },
        "prototyping": {
            "db": "FAISS",
            "reason": "单机运行，快速原型开发"
        },
        "production_small": {
            "db": "Milvus",
            "reason": "功能完整，Milvus Lite单机部署简单"
        },
        "production_large": {
            "db": "Milvus (分布式)",
            "reason": "支持大规模数据，高可用"
        },
        "cloud": {
            "db": "Pinecone/Zilliz Cloud",
            "reason": "完全托管，零运维"
        },
        "realtime": {
            "db": "Qdrant",
            "reason": "性能优异，支持实时更新"
        }
    }

    return recommendations.get(scenario, recommendations["learning"])
```

## 3.6 本章小结

本章介绍了向量数据库的相关知识：
- 主流向量数据库的对比
- Milvus的安装和基本操作
- FAISS索引类型和操作
- Chroma轻量级解决方案
- 根据场景选择合适的数据库

## 练习题

1. 在本地部署Milvus并创建知识库
2. 比较不同索引类型的检索效果
3. 实现一个基于FAISS的混合检索系统
4. 设计向量数据库的备份和恢复方案

## 参考资料

- [Milvus官方文档](https://milvus.io/docs/overview.md)
- [FAISS官方文档](https://faiss.ai/)
- [Chroma官方文档](https://docs.trychroma.com/)
- [VectorDB Benchmarks](https://github.com/rom1504/vector-db-benchmark)
