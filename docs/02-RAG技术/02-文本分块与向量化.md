# 第二章：文本分块与向量化

## 2.1 文本分块策略

### 2.1.1 为什么需要分块

- **长度限制**: LLM有上下文长度限制（如4K、8K、32K）
- **检索效率**: 小块文档更易于精确检索
- **语义完整**: 保持语义连贯性

### 2.1.2 固定大小分块

```python
class FixedSizeChunker:
    """固定大小分块器"""

    def __init__(self, chunk_size=500, chunk_overlap=50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def chunk_text(self, text):
        """将文本分割为固定大小的块"""
        words = text.split()
        chunks = []

        for i in range(0, len(words), self.chunk_size - self.chunk_overlap):
            chunk = ' '.join(words[i:i + self.chunk_size])
            chunks.append(chunk)

        return chunks

    def chunk_documents(self, documents):
        """批量分块"""
        all_chunks = []
        for doc in documents:
            chunks = self.chunk_text(doc)
            all_chunks.extend(chunks)
        return all_chunks


# 使用示例
chunker = FixedSizeChunker(chunk_size=200, chunk_overlap=20)
text = "人工智能是计算机科学的一个重要分支，它企图了解智能的实质，" \
       "并生产出一种新的能以人类智能相似的方式做出反应的智能机器。" \
       "研究范围包括机器学习、语音识别、自然语言处理等领域。"

chunks = chunker.chunk_text(text)
for i, chunk in enumerate(chunks):
    print(f"块{i+1}: {chunk} (长度: {len(chunk.split())}词)")
```

### 2.1.3 句子级分块

```python
import re
from typing import List

class SentenceChunker:
    """基于句子的分块器"""

    def __init__(self, chunk_size=500, chunk_overlap=50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def split_sentences(self, text):
        """分割句子"""
        # 中文句子分割（使用常见标点）
        sentences = re.split(r'[。！？；]', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        return sentences

    def chunk_text(self, text):
        """按句子分块"""
        sentences = self.split_sentences(text)
        chunks = []
        current_chunk = []
        current_length = 0

        for sentence in sentences:
            sentence_length = len(sentence)
            if current_length + sentence_length > self.chunk_size and current_chunk:
                chunks.append('。'.join(current_chunk) + '。')
                # 处理重叠
                overlap_start = max(0, len(current_chunk) - 3)
                current_chunk = current_chunk[overlap_start:]
                current_length = sum(len(s) for s in current_chunk)

            current_chunk.append(sentence)
            current_length += sentence_length

        if current_chunk:
            chunks.append('。'.join(current_chunk) + '。')

        return chunks
```

### 2.1.4 语义分块

```python
from typing import List, Tuple
import numpy as np

class SemanticChunker:
    """基于语义的智能分块器"""

    def __init__(self, embedding_model, threshold=0.7):
        self.embedding_model = embedding_model
        self.threshold = threshold

    def get_embedding(self, text):
        """获取文本嵌入"""
        return self.embedding_model.encode([text])[0]

    def calculate_similarity(self, vec1, vec2):
        """计算余弦相似度"""
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

    def find_breaks(self, sentences):
        """找到语义断点"""
        if len(sentences) <= 2:
            return [0]

        embeddings = [self.get_embedding(s) for s in sentences]
        breaks = [0]

        for i in range(1, len(embeddings)):
            sim = self.calculate_similarity(embeddings[i-1], embeddings[i])
            if sim < self.threshold:
                breaks.append(i)

        return breaks

    def chunk_text(self, text):
        """语义分块"""
        sentences = re.split(r'[。！？；]', text)
        sentences = [s.strip() for s in sentences if s.strip()]

        if len(sentences) < 2:
            return [text]

        breaks = self.find_breaks(sentences)

        chunks = []
        for i in range(len(breaks)):
            start = breaks[i]
            end = breaks[i + 1] if i + 1 < len(breaks) else len(sentences)
            chunk = '。'.join(sentences[start:end]) + '。'
            chunks.append(chunk)

        return [c for c in chunks if len(c) > 10]  # 过滤过短的块
```

### 2.1.5 Markdown/HTML结构分块

```python
from bs4 import BeautifulSoup
import re

class MarkdownChunker:
    """Markdown文档分块器"""

    def __init__(self, chunk_size=1000, chunk_overlap=100):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def extract_headers(self, text):
        """提取标题层级结构"""
        headers = []
        for i, line in enumerate(text.split('\n')):
            if line.startswith('#'):
                level = len(line) - len(line.lstrip('#'))
                headers.append({
                    'level': level,
                    'text': line.lstrip('# ').strip(),
                    'line': i
                })
        return headers

    def chunk_markdown(self, text):
        """按标题分块"""
        lines = text.split('\n')
        headers = self.extract_headers(text)

        chunks = []
        current_section = ""
        current_lines = []

        for i, line in enumerate(lines):
            # 检查是否是标题
            header = next((h for h in headers if h['line'] == i), None)

            if header:
                # 保存之前的section
                if current_section and current_lines:
                    chunk = '\n'.join(current_lines)
                    chunks.append(chunk)
                current_section = header['text']
                current_lines = [line]
            else:
                current_lines.append(line)

        # 保存最后一个section
        if current_lines:
            chunks.append('\n'.join(current_lines))

        # 进一步按大小分割
        final_chunks = []
        for chunk in chunks:
            if len(chunk) > self.chunk_size:
                words = chunk.split()
                for i in range(0, len(words), self.chunk_size - self.chunk_overlap):
                    sub_chunk = ' '.join(words[i:i + self.chunk_size])
                    final_chunks.append(sub_chunk)
            else:
                final_chunks.append(chunk)

        return final_chunks


class HTMLChunker:
    """HTML文档分块器"""

    def __init__(self):
        self.tag_weights = {
            'article': 1.0,
            'section': 0.9,
            'p': 0.8,
            'li': 0.7,
            'code': 0.6,
            'pre': 0.6
        }

    def extract_content(self, html):
        """提取HTML内容"""
        soup = BeautifulSoup(html, 'html.parser')

        # 移除脚本和样式
        for tag in soup(['script', 'style', 'nav', 'footer']):
            tag.decompose()

        # 提取文本
        text = soup.get_text(separator='\n')

        # 清理空白
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        return '\n'.join(lines)

    def chunk_html(self, html):
        """HTML分块"""
        text = self.extract_content(html)

        # 按段落分割
        paragraphs = text.split('\n\n')
        return [p.strip() for p in paragraphs if p.strip()]
```

## 2.2 文本向量化

### 2.2.1 词嵌入 vs 句子嵌入

```python
"""
词嵌入：捕捉单词的语义信息
- Word2Vec, GloVe
- 每个词一个向量

句子嵌入：捕捉整句的语义信息
- Sentence-BERT, E5
- 整句一个向量（用于检索）
"""
```

### 2.2.2 中文嵌入模型

```python
from sentence_transformers import SentenceTransformer

class ChineseEmbedder:
    """中文文本嵌入器"""

    def __init__(self, model_name="BAAI/bge-large-zh"):
        self.model = SentenceTransformer(model_name)

    def encode(self, texts, batch_size=32, normalize=True):
        """批量编码"""
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            normalize_embeddings=normalize,
            show_progress_bar=True
        )
        return embeddings

    def encode_single(self, text):
        """单条文本编码"""
        return self.model.encode([text], normalize_embeddings=True)[0]

    def compute_similarity(self, text1, text2):
        """计算两条文本的相似度"""
        emb1 = self.encode_single(text1)
        emb2 = self.encode_single(text2)
        return np.dot(emb1, emb2)

    def search(self, query, documents, top_k=5):
        """检索"""
        query_emb = self.encode_single(query)
        doc_embs = self.encode(documents)

        scores = np.dot(query_emb, doc_embs.T)
        top_indices = np.argsort(scores)[::-1][:top_k]

        return [(idx, documents[idx], scores[idx]) for idx in top_indices]


# 使用示例
embedder = ChineseEmbedder("BAAI/bge-large-zh")

texts = [
    "人工智能的发展历程",
    "机器学习算法介绍",
    "深度学习神经网络",
    "自然语言处理技术"
]

embeddings = embedder.encode(texts)
print("嵌入维度:", embeddings.shape)

# 检索
results = embedder.search("什么是深度学习？", texts, top_k=2)
for idx, text, score in results:
    print(f"相关度: {score:.4f}, 内容: {text}")
```

### 2.2.3 嵌入模型选择

| 模型 | 维度 | 特点 | 适用场景 |
|-----|------|------|---------|
| **BAAI/bge-large-zh** | 1024 | 中文效果好 | 中文检索 |
| **BAAI/bge-small-zh** | 512 | 轻量快速 | 资源受限 |
| **m3e-base** | 768 | 中英双语 | 多语言场景 |
| **text2vec-large** | 1024 | 语义相似度 | 匹配任务 |
| **GTE-large-zh** | 1024 | 通用效果好 | 通用检索 |

```python
# 多种嵌入模型对比
EMBEDDING_MODELS = {
    "bge-large-zh": {
        "dimension": 1024,
        "description": "中文效果好，适合专业文档"
    },
    "bge-small-zh": {
        "dimension": 512,
        "description": "轻量快速，适合实时场景"
    },
    "m3e-base": {
        "dimension": 768,
        "description": "中英双语，适合多语言"
    },
    "text2vec-base": {
        "dimension": 768,
        "description": "中文语义相似度"
    }
}

def select_embedding_model(task_type, resource_constraint="medium"):
    """根据任务选择嵌入模型"""
    if task_type == "chinese_rag":
        if resource_constraint == "low":
            return "BAAI/bge-small-zh"
        else:
            return "BAAI/bge-large-zh"
    elif task_type == "multilingual":
        return "intfloat/multilingual-e5-large"
    elif task_type == "semantic_similarity":
        return "shibing624/text2vec-base-chinese"
    else:
        return "BAAI/bge-base-zh"
```

## 2.3 分块策略对比

```python
class ChunkingBenchmark:
    """分块策略对比测试"""

    def __init__(self, embedder):
        self.embedder = embedder

    def evaluate_recall(self, chunker, documents, queries, relevant_docs):
        """评估检索召回率"""
        # 构建索引
        all_chunks = []
        for doc in documents:
            chunks = chunker.chunk_text(doc)
            all_chunks.extend(chunks)

        embeddings = self.embedder.encode(all_chunks)

        # 评估每个查询
        recalls = []
        for query, relevant in zip(queries, relevant_docs):
            query_emb = self.embedder.encode_single(query)
            scores = np.dot(query_emb, embeddings.T)
            top_k_indices = np.argsort(scores)[::-1][:10]

            # 计算召回
            retrieved_chunks = [all_chunks[i] for i in top_k_indices]

            # 简单判断：检索到的chunk是否包含相关信息
            hits = sum(1 for chunk in retrieved_chunks
                      if any(r in chunk for r in relevant))
            recalls.append(hits / len(relevant) if relevant else 0)

        return np.mean(recalls)

    def benchmark_chunkers(self, documents, queries, relevant_docs):
        """对比不同分块策略"""
        chunkers = {
            "fixed_size": FixedSizeChunker(300, 30),
            "sentence": SentenceChunker(300, 30),
        }

        results = {}
        for name, chunker in chunkers.items():
            recall = self.evaluate_recall(chunker, documents, queries, relevant_docs)
            results[name] = recall

        return results


# 使用示例
benchmark = ChunkingBenchmark(embedder)

documents = [...]  # 文档列表
queries = [...]   # 查询列表
relevant_docs = [...]  # 相关文档

results = benchmark.benchmark_chunkers(documents, queries, relevant_docs)
for name, score in results.items():
    print(f"{name}: 召回率 = {score:.4f}")
```

## 2.4 最佳实践

### 2.4.1 分块大小选择

```python
RECOMMENDED_CHUNK_SIZES = {
    "code": {
        "chunk_size": 512,
        "chunk_overlap": 50,
        "description": "代码块较小，保持函数完整性"
    },
    "paper": {
        "chunk_size": 1024,
        "chunk_overlap": 100,
        "description": "学术论文，段落级别"
    },
    "documentation": {
        "chunk_size": 768,
        "chunk_overlap": 80,
        "description": "技术文档，按标题分块"
    },
    "general": {
        "chunk_size": 512,
        "chunk_overlap": 50,
        "description": "通用文本"
    }
}
```

### 2.4.2 元数据增强

```python
class MetadataEnricher:
    """元数据增强器"""

    def __init__(self):
        import re
        from datetime import datetime

    def extract_metadata(self, text, source=None):
        """提取文档元数据"""
        metadata = {
            "source": source,
            "length": len(text),
            "word_count": len(text.split()),
            "paragraph_count": len(text.split('\n\n')),
            "extracted_at": datetime.now().isoformat()
        }

        # 提取标题（如果有）
        title_match = re.search(r'^# (.+)$', text, re.MULTILINE)
        if title_match:
            metadata["title"] = title_match.group(1)

        # 提取关键词
        words = text.split()
        if len(words) > 10:
            metadata["first_words"] = ' '.join(words[:10])

        return metadata

    def enrich_chunks(self, chunks, document_metadata):
        """为每个chunk添加元数据"""
        enriched = []
        for i, chunk in enumerate(chunks):
            enriched_chunk = {
                "content": chunk,
                "chunk_id": i,
                "chunk_length": len(chunk),
                **document_metadata
            }
            enriched.append(enriched_chunk)
        return enriched
```

## 2.5 本章小结

本章介绍了RAG系统中的文本预处理技术：
- 多种分块策略（固定大小、句子级、语义级、结构化）
- 文本向量化方法和模型选择
- 分块策略的评估和对比
- 元数据增强技术

## 练习题

1. 实现一个基于LDA主题的智能分块器
2. 比较不同嵌入模型在中文检索上的效果
3. 设计一个针对法律文档的专用分块策略
4. 实现分块的并行处理以提高效率

## 参考资料

- [Chunking Strategies for LLM Applications](https://github.com/FullStackRetrieval/comparison/blob/main/README.md) - 分块策略对比
- [Sentence-BERT](https://www.sbert.net/) - SBERT官方文档
- [BGE Embeddings](https://github.com/FlagOpen/FlagEmbedding) - BGE模型
