# 第二章：高效微调技术

## 2.1 概述

### 2.1.1 为什么需要微调

| 场景 | 适用方案 | 说明 |
|-----|---------|------|
| 通用对话 | 预训练模型 | 开箱即用 |
| 特定领域知识 | RAG | 注入知识库 |
| 特定格式输出 | Prompt工程 | 零样本调优 |
| 特定行为风格 | SFT微调 | 监督学习 |
| 对齐人类偏好 | RLHF | 基于反馈优化 |

### 2.1.2 微调方法对比

| 方法 | 可训练参数 | 显存占用 | 精度损失 | 训练速度 |
|-----|-----------|---------|---------|---------|
| **全量微调** | 所有 | 极高 | 无 | 慢 |
| **LoRA** | 1-5% | 中等 | 极小 | 快 |
| **QLoRA** | 1-5% | 低 | 较小 | 中 |
| **Prefix Tuning** | <1% | 低 | 较小 | 快 |
| **Adapter** | 1-3% | 低 | 较小 | 快 |

## 2.2 PEFT库使用

### 2.2.1 LoRA配置

```python
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType

# 加载模型
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto"
)

# LoRA配置
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=16,                    # LoRA秩
    lora_alpha=32,           # LoRA缩放因子
    lora_dropout=0.05,       # Dropout比例
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]  # 目标模块
)

# 应用LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# 输出: trainable params: 6,291,456 || all params: 7,634,923,520 || trainable%: 0.0824
```

### 2.2.2 QLoRA配置

```python
from transformers import AutoModelForCausalLM
from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model
from bitsandbytes import BitsAndBytesConfig

# 4-bit量化配置
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",      # NormalFloat4
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,  # 双重量化节省显存
)

# 加载量化模型
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=bnb_config,
    device_map="auto"
)

# 准备训练
model = prepare_model_for_int8_training(model)

# 应用LoRA
lora_config = LoraConfig(
    r=64,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
```

### 2.2.3 训练配置

```python
from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq

# 数据整理器
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    padding=True
)

# 训练参数
training_args = TrainingArguments(
    output_dir="./output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=8,    # 梯度累积
    learning_rate=2e-5,
    weight_decay=0.01,
    warmup_steps=100,
    max_grad_norm=1.0,               # 梯度裁剪
    lr_scheduler_type="cosine",
    logging_steps=10,
    save_steps=500,
    save_total_limit=3,
    fp16=True,                        # 混合精度
    optim="adamw_torch",
    report_to="tensorboard",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
)

# 训练器
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
    tokenizer=tokenizer,
)

# 开始训练
trainer.train()
```

## 2.3 数据准备

### 2.3.1 指令数据格式

```python
# Alpaca格式
{
    "instruction": "用户指令",
    "input": "输入内容（可选）",
    "output": "期望输出"
}

# ShareGPT格式
{
    "conversations": [
        {"from": "human", "value": "用户输入"},
        {"from": "gpt", "value": "模型输出"}
    ]
}

# OpenAI格式
{
    "messages": [
        {"role": "system", "content": "系统提示"},
        {"role": "user", "content": "用户输入"},
        {"role": "assistant", "content": "期望输出"}
    ]
}
```

### 2.3.2 数据集创建

```python
from datasets import Dataset

def create_instruction_dataset(data_path):
    """创建指令微调数据集"""
    # 读取数据
    import json
    with open(data_path, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)

    # 转换为标准格式
    formatted_data = []
    for item in raw_data:
        # 构造对话
        messages = []

        if "system" in item:
            messages.append({"role": "system", "content": item["system"]})

        if "instruction" in item:
            user_content = item["instruction"]
            if "input" in item and item["input"]:
                user_content += f"\n\n{item['input']}"
            messages.append({"role": "user", "content": user_content})

        if "output" in item:
            messages.append({"role": "assistant", "content": item["output"]})

        formatted_data.append({"messages": messages})

    # 创建Dataset
    dataset = Dataset.from_list(formatted_data)
    return dataset


def create_chat_dataset(conversations):
    """从对话列表创建数据集"""
    data = []
    for conv in conversations:
        messages = []
        for turn in conv:
            messages.append({
                "role": turn["role"],
                "content": turn["content"]
            })
        data.append({"messages": messages})

    return Dataset.from_list(data)
```

### 2.3.3 数据预处理

```python
def preprocess_function(examples, tokenizer, max_length=2048):
    """预处理函数"""
    # 获取messages
    messages_list = examples["messages"]

    # 使用tokenizer的chat template
    texts = []
    for messages in messages_list:
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        texts.append(text)

    # Tokenize
    inputs = tokenizer(
        texts,
        max_length=max_length,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )

    # 设置labels
    inputs["labels"] = inputs["input_ids"].clone()

    # 对于assistant的token保留labels，其他位置设为-100
    # 这里简化处理：保留所有token的labels
    # 实际应用中应该只在assistant回复部分计算loss

    return inputs


# 应用预处理
train_dataset = dataset.map(
    preprocess_function,
    fn_kwargs={"tokenizer": tokenizer},
    batched=True,
    remove_columns=["messages"]
)
```

## 2.4 模型导出

### 2.4.1 合并LoRA权重

```python
from peft import PeftModel
from transformers import AutoModelForCausalLM

base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto"
)

# 加载LoRA权重
lora_model = PeftModel.from_pretrained(
    base_model,
    "./output/checkpoint-500"
)

# 合并权重
merged_model = lora_model.merge_and_unload()

# 保存合并后的模型
merged_model.save_pretrained("./output/merged_model")
tokenizer.save_pretrained("./output/merged_model")
```

### 2.4.2 量化导出

```python
# 使用GPTQ量化
from transformers import GPTQQuantizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "./output/merged_model",
    torch_dtype=torch.float16,
    device_map="auto"
)

# GPTQ量化配置
quantizer = GPTQQuantizer(
    bits=4,
    dataset="c4",
    tokenizer=tokenizer
)

# 量化
quantized_model = quantizer.quantize_model(model, quantization_config=None)

# 保存
quantized_model.save_pretrained("./output/quantized_model")
```

### 2.4.3 GGUF导出

```python
# 使用llama.cpp转换
# pip install gguf

import gguf

# 加载模型
model = AutoModelForCausalLM.from_pretrained(
    "./output/merged_model",
    torch_dtype=torch.float16
)

# 转换为GGUF
gguf_writer = gguf.GGUFWriter(
    path="./output/model.gguf",
    arch="llama"
)

# 添加模型信息
gguf_writer.add_meta_data("title", "Fine-tuned Qwen Model")

# 转换权重
gguf_writer.write_model_to_file(model, tokenizer)

gguf_writer.close()
```

## 2.5 分布式训练

### 2.5.1 DeepSpeed配置

```python
# deepspeed_config.json
{
    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "contiguous_gradients": true,
        "overlap_comm": true
    },
    "train_batch_size": 32,
    "gradient_accumulation_steps": 4,
    "steps_per_print": 10,
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 0.001,
            "betas": [0.9, 0.999],
            "eps": 1e-8
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 0.001,
            "warmup_num_steps": 100
        }
    }
}
```

### 2.5.2 使用DeepSpeed训练

```python
from transformers import TrainingArguments, Trainer
import deepspeed

# 训练参数
training_args = TrainingArguments(
    output_dir="./output",
    deepspeed="./deepspeed_config.json",
    # ... 其他参数
)

# 使用DeepSpeed插件初始化
from accelerate import Accelerator

accelerator = Accelerator()

model, optimizer, dataloader = accelerator.prepare(
    model, optimizer, dataloader
)
```

### 2.5.3 FSDP配置

```python
# 使用FSDP
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./output",
    fsdp="full_shard auto_wrap",
    fsdp_config={
        "backward_prefetch": "pre_forward",
        "forward_prefetch": True,
        "activation_checkpointing": True,
    },
)
```

## 2.6 本章小结

本章介绍了高效微调技术：
- PEFT库LoRA/QLoRA配置
- 数据准备和预处理
- 模型导出和量化
- 分布式训练配置

## 练习题

1. 使用LoRA微调一个7B模型
2. 比较全量微调和LoRA的效果差异
3. 实现QLoRA在消费级显卡上训练
4. 导出微调后的模型并部署

## 参考资料

- [PEFT Documentation](https://huggingface.co/docs/peft)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [QLoRA Paper](https://arxiv.org/abs/2305.14389)
- [DeepSpeed Documentation](https://www.deepspeed.ai/)
