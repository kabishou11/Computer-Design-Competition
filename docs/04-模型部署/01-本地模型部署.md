# 第一章：本地模型部署

## 1.1 部署方式概述

### 1.1.1 常见部署方式

| 方式 | 优点 | 缺点 | 适用场景 |
|-----|------|------|---------|
| **Transformers** | 简单灵活 | 显存占用高 | 开发测试 |
| **vLLM** | 高吞吐、高并发 | 功能有限 | 生产部署 |
| **llama.cpp** | 纯CPU、资源少 | 速度较慢 | 边缘部署 |
| **Text Generation WebUI** | 界面友好 | 功能受限 | 快速体验 |
| **Ollama** | 简单易用 | 自定义受限 | 个人使用 |

### 1.1.2 硬件配置参考

| 模型大小 | 显存要求 | 推荐部署方式 |
|---------|----------|-------------|
| 1.5B-3B | 6-8GB | Transformers/vLLM |
| 7B | 16-24GB | Transformers/vLLM |
| 13B | 28-40GB | vLLM |
| 30B+ | 64GB+ | vLLM/分布式 |

## 1.2 Transformers推理

### 1.2.1 基本推理

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

class LocalLLM:
    """本地LLM推理"""

    def __init__(self, model_name, device="cuda"):
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )

    def generate(self, prompt, max_tokens=512, temperature=0.7, top_p=0.9):
        """生成文本"""
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        response = self.tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )

        return response

    def chat(self, messages, max_tokens=512):
        """对话模式"""
        # 转换为对话格式
        prompt = self._format_messages(mrompt)

        response = self.generate(prompt, max_tokens)
        return response

    def _format_messages(self, messages):
        """格式化消息"""
        if isinstance(messages, list):
            text = ""
            for msg in messages:
                role = msg["role"]
                content = msg["content"]
                text += f"<|im_start|>{role}\n{content}<|im_end|>\n"
            text += "<|im_start|>assistant\n"
            return text
        return messages


# 使用示例
model = LocalLLM("Qwen/Qwen2.5-7B-Instruct")

response = model.generate("介绍一下人工智能的发展历程。")
print(response)

# 对话模式
messages = [
    {"role": "system", "content": "你是一个专业的AI助手。"},
    {"role": "user", "content": "什么是深度学习？"}
]
response = model.chat(messages)
print(response)
```

### 1.2.2 批量推理

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.utils.data import Dataset, DataLoader
import torch

class InferenceDataset(Dataset):
    """推理数据集"""

    def __init__(self, prompts, tokenizer, max_length=512):
        self.prompts = prompts
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.prompts)

    def __getitem__(self, idx):
        prompt = self.prompts[idx]
        inputs = self.tokenizer(
            prompt,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        return inputs


def batch_inference(model, tokenizer, prompts, batch_size=4, max_tokens=100):
    """批量推理"""
    dataset = InferenceDataset(prompts, tokenizer)
    dataloader = DataLoader(dataset, batch_size=batch_size)

    results = []

    for batch in dataloader:
        input_ids = batch["input_ids"].to(model.device)
        attention_mask = batch["attention_mask"].to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_tokens,
                pad_token_id=tokenizer.eos_token_id
            )

        for i in range(len(outputs)):
            generated = outputs[i]
            input_len = input_ids[i].shape[0]
            response = tokenizer.decode(generated[input_len:], skip_special_tokens=True)
            results.append(response)

    return results
```

## 1.3 vLLM部署

### 1.3.1 安装和启动

```bash
# 安装vLLM
pip install vllm

# 启动模型服务
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-7B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 1 \
    --dtype half
```

### 1.3.2 OpenAI兼容API

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="sk-no-key-required"
)

# 文本补全
response = client.completions.create(
    model="Qwen/Qwen2.5-7B-Instruct",
    prompt="介绍一下人工智能",
    max_tokens=512,
    temperature=0.7
)

print(response.choices[0].text)

# 对话补全
chat_response = client.chat.completions.create(
    model="Qwen/Qwen2.5-7B-Instruct",
    messages=[
        {"role": "system", "content": "你是一个AI助手。"},
        {"role": "user", "content": "什么是机器学习？"}
    ],
    temperature=0.7,
    max_tokens=512
)

print(chat_response.choices[0].message.content)
```

### 1.3.3 批量请求

```python
import asyncio
import httpx

class AsyncBatchClient:
    """异步批量请求客户端"""

    def __init__(self, base_url="http://localhost:8000/v1"):
        self.base_url = base_url
        self.client = httpx.AsyncClient(timeout=300.0)

    async def generate(self, prompt, max_tokens=512):
        """单次生成"""
        response = await self.client.post(
            f"{self.base_url}/completions",
            json={
                "model": "Qwen/Qwen2.5-7B-Instruct",
                "prompt": prompt,
                "max_tokens": max_tokens
            }
        )
        return response.json()

    async def batch_generate(self, prompts, max_tokens=512):
        """批量生成"""
        tasks = [self.generate(p, max_tokens) for p in prompts]
        results = await asyncio.gather(*tasks)
        return results


async def main():
    client = AsyncBatchClient()

    prompts = [
        "什么是AI？",
        "什么是ML？",
        "什么是DL？"
    ]

    results = await client.batch_generate(prompts)
    for r in results:
        print(r["choices"][0]["text"])


asyncio.run(main())
```

## 1.4 llama.cpp部署

### 1.4.1 模型转换

```bash
# 安装llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make

# 下载GGUF格式模型
# 从HuggingFace下载：https://huggingface.co/TheBloke/Qwen-7B-Chat-GGUF

# 或转换模型为GGUF格式
python convert-hf-to-gguf.py /path/to/hf/model --outtype f16
```

### 1.4.2 启动服务

```bash
# 启动HTTP服务
./llama-server \
    -m Qwen-7B-Chat.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --temp 0.7 \
    --threads 4
```

### 1.4.3 Python客户端

```python
import requests

class LlamaCppClient:
    """llama.cpp客户端"""

    def __init__(self, base_url="http://localhost:8080"):
        self.base_url = base_url

    def generate(self, prompt, **kwargs):
        """生成"""
        response = requests.post(
            f"{self.base_url}/completion",
            json={
                "prompt": prompt,
                **kwargs
            }
        )
        return response.json()

    def chat(self, messages, **kwargs):
        """对话"""
        # 转换为prompt
        prompt = self._format_messages(messages)

        return self.generate(prompt, **kwargs)

    def _format_messages(self, messages):
        """格式化消息"""
        prompt = ""
        for msg in messages:
            role = msg["role"]
            content = msg["content"]
            prompt += f"[{role}] {content}\n"
        prompt += "[assistant] "
        return prompt


# 使用示例
client = LlamaCppClient()

response = client.generate(
    "介绍一下AI",
    max_tokens=512,
    temperature=0.7,
    stop=["[user]"]
)
print(response["content"])
```

## 1.5 Ollama部署

### 1.5.1 安装和使用

```bash
# 安装Ollama
# macOS: brew install ollama
# Linux: curl -fsSL https://ollama.ai/install.sh | sh
# Windows: 下载安装包

# 拉取模型
ollama pull qwen2:7b

# 运行模型
ollama run qwen2:7b

# 自定义Modelfile
FROM qwen2:7b

# 设置系统提示
SYSTEM """你是一个专业的AI助手。"""

# 设置参数
PARAMETER temperature 0.7
PARAMETER top_k 50

# 保存为新模型
ollama save -f my-assistant my-assistant
```

### 1.5.2 Ollama API

```python
import requests

class OllamaClient:
    """Ollama客户端"""

    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url

    def generate(self, model, prompt, **kwargs):
        """生成"""
        response = requests.post(
            f"{self.base_url}/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "stream": False,
                **kwargs
            }
        )
        return response.json()

    def chat(self, model, messages, **kwargs):
        """对话"""
        # 转换为单轮prompt
        prompt = ""
        for msg in messages:
            if msg["role"] == "system":
                continue
            prompt += f"{msg['role']}: {msg['content']}\n"
        prompt += "assistant: "

        return self.generate(model, prompt, **kwargs)

    def list_models(self):
        """列出模型"""
        response = requests.get(f"{self.base_url}/api/tags")
        return response.json()


# 使用示例
client = OllamaClient()

response = client.generate(
    model="qwen2:7b",
    prompt="什么是人工智能？",
    options={
        "temperature": 0.7,
        "num_predict": 512
    }
)
print(response["response"])
```

## 1.6 API服务封装

### 1.6.1 FastAPI服务

```python
# server.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import uvicorn

app = FastAPI(title="LLM API Server")

class GenerationRequest(BaseModel):
    prompt: str
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    max_tokens: int = 512
    temperature: float = 0.7

class GenerationResponse(BaseModel):
    text: str
    usage: dict

# 初始化模型
llm = None

@app.on_event("startup")
async def startup():
    global llm
    from transformers import AutoModelForCausalLM, AutoTokenizer
    llm = LocalLLM("Qwen/Qwen2.5-7B-Instruct")

@app.post("/generate")
async def generate(request: GenerationRequest):
    """文本生成"""
    response = llm.generate(
        prompt=request.prompt,
        max_tokens=request.max_tokens,
        temperature=request.temperature,
        top_p=request.top_p
    )
    return GenerationResponse(text=response, usage={})

@app.post("/chat")
async def chat(request: ChatRequest):
    """对话生成"""
    response = llm.chat(
        messages=[m.dict() for m in request.messages],
        max_tokens=request.max_tokens
    )
    return GenerationResponse(text=response, usage={})

@app.get("/health")
async def health():
    """健康检查"""
    return {"status": "ok"}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 1.6.2 启动服务

```bash
# 安装依赖
pip install fastapi uvicorn transformers torch

# 启动服务
python server.py

# 或使用gunicorn
gunicorn server:app -w 4 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000
```

## 1.7 本章小结

本章介绍了多种本地模型部署方式：
- Transformers直接推理
- vLLM高并发服务
- llama.cpp轻量部署
- Ollama简单易用
- FastAPI服务封装

## 练习题

1. 使用Transformers加载一个7B模型并推理
2. 使用vLLM搭建高并发API服务
3. 比较不同部署方式的性能差异
4. 实现一个完整的FastAPI服务

## 参考资料

- [vLLM Documentation](https://docs.vllm.ai/)
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [Ollama](https://ollama.ai/)
- [Hugging Face Inference](https://huggingface.co/docs/inference-overview)
