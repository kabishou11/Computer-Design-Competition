# 第四章：从词向量到大语言模型

## 4.1 词向量时代（2013-2017）

### 4.1.1 Word2Vec的诞生背景

2013年，Mikolov在Google发表Word2Vec，这是一个里程碑事件。

**历史背景**：
- 神经网络语言模型（2003）理论上可行，但计算成本高
- 需要更简单、更高效的方法
- 分布式假说早就存在，缺少工程实现

**Word2Vec的创新**：
- 极其简单的模型结构
- 超大规模训练（数十亿词）
- 开源工具包

### 4.1.2 Word2Vec的两种架构

**CBOW（Continuous Bag-of-Words）**：

用上下文词预测中心词：

$$\text{目标} = \sum_{w \in \text{context}} \log P(w | \text{center})$$

**Skip-gram**：

用中心词预测上下文词：

$$\text{目标} = \sum_{w \in \text{context}} \log P(\text{center} | w)$$

**为什么Skip-gram更流行？**

- 对于罕见词效果好
- 训练目标更直接
- GitHub上star更多

### 4.1.3 词向量的经典应用

1. **词语相似度**：
$$\text{sim}(a, b) = \cos(v_a, v_b) = \frac{v_a \cdot v_b}{|v_a||v_b|}$$

2. **类比推理**：
$$v_{\text{king}} - v_{\text{man}} + v_{\text{woman}} \approx v_{\text{queen}}$$

3. **聚类分析**：对词向量聚类发现语义类别

## 4.2 预训练词向量（2014-2018）

### 4.2.1 GloVe

**GloVe（Global Vectors）** 结合了全局矩阵分解和局部上下文窗口。

**目标函数**：

$$J = \sum_{i,j=1}^{V} f(X_{ij})(w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2$$

其中 $X_{ij}$ 是词 $i$ 和词 $j$ 的共现次数，$f$ 是加权函数。

### 4.2.2 FastText

**FastText** 改进了Word2Vec：
- 使用子词（subword）信息
- 解决OOV（Out-of-Vocabulary）问题
- 每个词表示为字符n-gram的和

**公式**：

$$v_w = \sum_{g \in G_w} z_g$$

其中 $G_w$ 是词 $w$ 的所有n-gram集合，$z_g$ 是n-gram的向量。

## 4.3 上下文词向量（2018-2019）

### 4.3.1 ELMo：双向LSTM

**ELMo（Embeddings from Language Models）** 核心思想：

同一个词在不同上下文中有不同的向量。

**双向LSTM**：

$$\text{forward: } p(w_1, ..., w_n) = \prod_{i=1}^{n} p(w_i | w_1, ..., w_{i-1})$$

$$\text{backward: } p(w_1, ..., w_n) = \prod_{i=1}^{n} p(w_i | w_{i+1}, ..., w_n)$$

**ELMo表示**：

$$ELMo_w = \gamma(w) [h_{LSTM1}[w]; h_{LSTM2}[w]]$$

### 4.3.2 ULMFiT：预训练+微调

**ULMFiT** 提出"预训练-微调"范式：

1. **在大规模语料上预训练语言模型**
2. **在目标任务上微调**

三种微调技术：
- **判别式微调**：不同层用不同学习率
- **倾斜三角学习率**：先增后减
- **逐步解冻**：逐层解冻避免灾难性遗忘

## 4.4 BERT时代（2018-2020）

### 4.4.1 BERT的诞生

**BERT（Bidirectional Encoder Representations from Transformers）** 2018年发表。

**核心创新**：
1. **双向Transformer**：同时考虑左右上下文
2. **掩码语言建模**：随机掩盖15%的词
3. **下一句预测**：学习句子级关系

### 4.4.2 BERT的预训练任务

**MLM目标**：

$$\mathcal{L}_{MLM} = \mathbb{E}_{(M, X) \sim D} [\log P(w_M | X_{\setminus M})]$$

其中 $M$ 是掩盖位置，$X_{\setminus M}$ 是未掩盖的token。

**掩盖策略**：
- 80%：替换为 [MASK]
- 10%：替换为随机词
- 10%：保持不变

### 4.4.3 BERT的影响

**GLUE基准**：评估语言理解能力

| 模型 | GLUE分数 | 参数量 |
|-----|---------|--------|
| BERT-Base | 78.9 | 110M |
| BERT-Large | 81.3 | 340M |
| RoBERTa | 88.5 | 355M |

**下游任务微调**：
- 分类、问答、命名实体识别等

## 4.5 GPT系列（2018-2023）

### 4.5.1 GPT-1：开创性的自回归模型

**GPT（Generative Pre-trained Transformer）** 采用解码器-only架构。

**预训练目标**：标准语言建模

$$\mathcal{L}_{LM} = \sum_i \log P(w_i | w_1, ..., w_{i-1})$$

### 4.5.2 GPT-2：零样本能力涌现

**GPT-2** 展示"涌现能力"（Emergent Abilities）：

> 当模型规模增大到临界点，突然获得小模型不具备的能力

**零样本提示**：
不需要任何微调，直接通过提示完成任务。

### 4.5.3 GPT-3：少样本学习

**GPT-3** 175B参数，展示了**上下文学习**（In-Context Learning）：

$$\text{输出} = P(\text{输出} | \text{提示})$$

**提示工程（Prompt Engineering）** 变得重要：
- 少样本示例
- 任务描述
- 输出格式

### 4.5.4 InstructGPT：RLHF

**InstructGPT** 引入RLHF（基于人类反馈的强化学习）：

1. **SFT**：监督微调GPT
2. **奖励模型**：训练奖励模型预测人类偏好
3. **PPO**：用PPO算法优化GPT

## 4.6 大语言模型时代（2020-至今）

### 4.6.1 模型规模的演进

| 模型 | 年份 | 参数量 |
|-----|------|-------|
| GPT-2 | 2019 | 1.5B |
| GPT-3 | 2020 | 175B |
| PaLM | 2022 | 540B |
| GPT-4 | 2023 | 估计1-10T |

### 4.6.2 涌现能力

**研究表明**，以下能力在模型规模达到临界点时涌现：

1. **算术推理**：多位加减乘除
2. **逻辑推理**：复杂逻辑判断
3. **世界知识**：事实性问答
4. **代码生成**：编程任务
5. **多语言能力**：翻译、跨语言理解

### 4.6.3 缩放定律（Scaling Laws）

**Kaplan et al., 2021** 发现：

$$\text{损失} \propto N^{-\alpha} + D^{-\beta}$$

其中 $N$ 是参数量，$D$ 是数据量。

**结论**：
- 损失随参数量幂律下降
- 训练数据量应与参数量成比例

### 4.6.4 2024年新趋势

1. **开源崛起**：LLaMA、Mistral、Qwen
2. **多模态**：GPT-4V、Gemini
3. **Agent化**：GPTs、Agents
4. **专业化**：CodeLlama、MedPaLM

## 4.7 本章小结

| 阶段 | 关键突破 | 代表工作 | 核心思想 |
|-----|---------|---------|---------|
| 2013 | Word2Vec | Mikololov | 分布式表示 |
| 2018 | BERT | Google | 双向预训练 |
| 2018-2019 | GPT系列 | OpenAI | 自回归预训练 |
| 2020-至今 | 涌现能力 | GPT-3/4 | 规模效应 |

**核心脉络**：
1. 从独立词向量到上下文词向量
2. 从任务特定到预训练-微调
3. 从有监督到自监督
4. 从小模型到超大规模

## 参考文献

1. Mikolov, T., et al. (2013). Efficient Estimation of Word Representations in Vector Space.
2. Devlin, J., et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers.
3. Radford, A., et al. (2019). Language Models are Few-Shot Learners.
4. Kaplan, J., et al. (2021). Scaling Laws for Neural Language Models.
5. Wei, J., et al. (2022). Emergent Abilities of Large Language Models.
