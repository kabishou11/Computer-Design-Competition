# 第一章：语言模型的数学基础

## 1.1 从概率论到语言模型

### 1.1.1 什么是语言模型

语言模型（Language Model）的本质是一个**概率分布**。给定一个词序列 $w_1, w_2, ..., w_n$，语言模型计算这个序列出现的概率：

$$P(w_1, w_2, ..., w_n) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_1, w_2) \cdot ... \cdot P(w_n|w_1, ..., w_{n-1})$$

这个分解是概率论中著名的**链式法则**（Chain Rule）。

**关键洞察**：语言模型的核心任务是**预测下一个词**。给定前面的词，预测下一个词出现的概率分布。

$$P(w_t | w_1, w_2, ..., w_{t-1})$$

### 1.1.2 N-gram语言模型

最早的统计语言模型是N-gram模型，其核心思想是**马尔可夫假设**：

> 当前词只依赖于前面N-1个词

**二元语法（Bigram）**：
$$P(w_i | w_1, ..., w_{i-1}) \approx P(w_i | w_{i-1})$$

**三元语法（Trigram）**：
$$P(w_i | w_1, ..., w_{i-1}) \approx P(w_i | w_{i-2}, w_{i-1})$$

**N-gram的参数估计（最大似然估计MLE）**：

$$P(w_i | w_{i-N+1}^{i-1}) = \frac{count(w_{i-N+1}^{i-1}, w_i)}{count(w_{i-N+1}^{i-1})}$$

其中 $count(\cdot)$ 表示在语料库中出现的次数。

### 1.1.3 词向量的本质：分布式假说

1957年，Harris提出**分布式假说**（Distributional Hypothesis）：

> "You shall know a word by the company it keeps"（观其伴，知其意）

这句话揭示了**词向量**的哲学基础：

- **传统方法**：一个词是一个独立的符号（One-Hot编码）
- **分布式表示**：一个词由其上下文定义

**One-Hot编码的问题**：

假设词表大小为V=10000：
- "猫" → [0, 0, 0, ..., 1, ..., 0]（只有第i个位置为1）
- "狗" → [0, 0, 1, ..., 0, ..., 0]（只有第j个位置为1）

这两个向量是正交的，无法表示"猫"和"狗"的语义相似性。

## 1.2 词向量的数学原理

### 1.2.1 共现矩阵（Co-occurrence Matrix）

最早的分布式表示方法之一是**基于共现矩阵的词向量**。

给定语料库，构建矩阵 $M \in \mathbb{R}^{V \times V}$，其中 $M_{ij}$ 表示词 $w_i$ 和 $w_j$ 在固定窗口内共同出现的次数。

**示例**：
```
语料库：I like deep learning. I like NLP. Deep learning is fun.
```

共现矩阵：
```
        I   like   deep   learning   NLP   is    fun
I       0     2      0       0        0     0     0
like    2     0      2       0        1     0     0
deep    0     2      0       2        0     0     0
learning 0     0      2       0        0     0     1
NLP     0     1      0       0        0     0     0
is      0     0      0       0        0     0     1
fun     0     0      0       1        0     1     0
```

**问题**：高频词（如"the"）会主导向量空间。

**解决方案**：使用PPMI（Positive Pointwise Mutual Information）

$$PPMI(w_i, w_j) = \max\left(0, PMI(w_i, w_j)\right)$$

$$PMI(w_i, w_j) = \log\frac{P(w_i, w_j)}{P(w_i) \cdot P(w_j)}$$

**推导**：
- $P(w_i, w_j) = \frac{count(w_i, w_j)}{N}$ （联合概率）
- $P(w_i) = \frac{count(w_i)}{N}$ （边缘概率）

PMI衡量的是两个词之间的**关联程度**：
- PMI > 0：正相关（经常一起出现）
- PMI < 0：负相关（很少一起出现）
- PMI = 0：相互独立

### 1.2.2 SVD降维

共现矩阵维度为 $V \times V$（V通常为几万到几十万），需要降维。

**奇异值分解（SVD）**：

$$M = U \Sigma V^T$$

其中：
- $U \in \mathbb{R}^{V \times k}$：左奇异向量矩阵
- $\Sigma \in \mathbb{R}^{k \times k}$：对角矩阵，包含前k个奇异值
- $V^T \in \mathbb{R}^{k \times V}$：右奇异向量矩阵

**词向量**：取 $U$ 或 $V$ 的前k列作为词向量

$$w_i \approx U_i \cdot \Sigma$$

### 1.2.3 Word2Vec的数学原理

2013年，Mikolov提出Word2Vec，包括两个模型：

1. **CBOW（Continuous Bag-of-Words）**：用上下文预测中心词
2. **Skip-gram**：用中心词预测上下文

**Skip-gram的目标函数**：

给定训练语料库中的词序列 $w_1, w_2, ..., w_T$，目标是最小化：

$$J = \frac{1}{T}\sum_{t=1}^{T}\sum_{-c \leq j \leq c, j \neq 0} \log P(w_{t+j} | w_t)$$

其中：
- $c$：上下文窗口大小
- $P(w_{t+j} | w_t)$：给定中心词 $w_t$，预测上下文词 $w_{t+j}$ 的概率

**softmax函数**：

$$P(w_O | w_I) = \frac{\exp(v'_{w_O}^T v_{w_I})}{\sum_{w=1}^{V} \exp(v'_w^T v_{w_I})}$$

其中：
- $v_{w_I}$：输入词向量（中心词）
- $v'_{w_O}$：输出词向量（上下文词）

**问题**：softmax计算复杂度为 $O(V)$，词表通常为几十万到几百万。

**解决方案**：负采样（Negative Sampling）

**负采样目标**：

对于一个正样本 $(w_t, w_{t+j})$，采样k个负样本 $(w_t, w_{neg_1}), ..., (w_t, w_{neg_k})$

$$\log \sigma(v'_{w_O}^T v_{w_I}) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)}[\log \sigma(-v'_{w_i}^T v_{w_I})]$$

其中 $\sigma(x) = \frac{1}{1 + e^{-x}}$ 是sigmoid函数。

## 1.3 信息论基础

### 1.3.1 熵（Entropy）

熵衡量随机变量的不确定性：

$$H(X) = -\sum_{x} P(x) \log_2 P(x)$$

**直观理解**：
- 熵越大，不确定性越高
- 分布越均匀，熵越大
- 完全确定的分布，熵为0

**示例**：抛硬币
- 公平硬币：$H = -0.5\log_2(0.5) - 0.5\log_2(0.5) = 1$ bit
- 完全偏向正面：$H = 0$ bit

### 1.3.2 交叉熵（Cross Entropy）

交叉熵衡量两个概率分布的差异：

$$H(P, Q) = -\sum_{x} P(x) \log_2 Q(x)$$

语言模型常用交叉熵作为损失函数：

$$CE = -\frac{1}{N}\sum_{i=1}^{N} \log P(w_i | w_1, ..., w_{i-1})$$

**困惑度（Perplexity）**：

$$PPL = 2^{H(P, \hat{P})}$$

困惑度可以理解为"模型预测下一个词时，平均考虑多少个候选词"。

### 1.3.3 KL散度

KL散度衡量分布Q相对于分布P的"信息损失"：

$$D_{KL}(P || Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}$$

**性质**：
- $D_{KL}(P || Q) \geq 0$（吉布斯不等式）
- $D_{KL}(P || Q) \neq D_{KL}(Q || P)$（不对称）

**与交叉熵的关系**：

$$H(P, Q) = H(P) + D_{KL}(P || Q)$$

当 $P$ 固定时，最小化交叉熵等价于最小化KL散度。

## 1.4 语言模型的训练理论

### 1.4.1 最大似然估计（MLE）

语言模型的参数 $\theta$ 通过最大化似然函数学习：

$$\theta^* = \arg\max_\theta \prod_{t=1}^{T} P(w_t | w_1, ..., w_{t-1}; \theta)$$

取对数：

$$\theta^* = \arg\max_\theta \sum_{t=1}^{T} \log P(w_t | w_1, ..., w_{t-1}; \theta)$$

**梯度下降**：

$$\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta)$$

其中 $\mathcal{L}(\theta) = -\sum_{t=1}^{T} \log P(w_t | \cdot; \theta)$

### 1.4.2 自回归语言模型

自回归模型（Autoregressive Model）按顺序生成文本：

$$P(x) = P(x_1) \cdot P(x_2|x_1) \cdot P(x_3|x_1, x_2) \cdot ... \cdot P(x_n|x_1, ..., x_{n-1})$$

**优点**：
- 可以精确计算似然
- 生成质量高

**缺点**：
- 不能并行计算（生成时必须逐词预测）
- 计算复杂度与序列长度成正比

### 1.4.3 双向语言模型

BERT使用双向模型，同时考虑左右上下文：

$$P(w_i | w_1, ..., w_{i-1}, w_{i+1}, ..., w_n)$$

**掩码语言建模（MLM）目标**：

随机掩盖15%的词，预测被掩盖的词：

$$\mathcal{L} = -\mathbb{E}_{m \sim M} [\log P(w_m | w_{\setminus m})]$$

## 1.5 本章小结

本章介绍了语言模型的数学基础：

| 概念 | 数学表达 | 核心思想 |
|-----|---------|---------|
| 链式法则 | $P(w_1,...,w_n) = \prod P(w_i|w_1...w_{i-1})$ | 分解联合概率 |
| 马尔可夫假设 | $P(w_i|w_1...w_{i-1}) \approx P(w_i|w_{i-N+1}...w_{i-1})$ | 简化依赖 |
| 熵 | $H(X) = -\sum P(x)\log P(x)$ | 衡量不确定性 |
| 交叉熵 | $H(P,Q) = -\sum P(x)\log Q(x)$ | 衡量分布差异 |
| KL散度 | $D_{KL}(P\|\|Q)$ | 衡量信息损失 |

理解这些数学基础对于深入理解Transformer和大语言模型至关重要。

## 参考文献

1. Shannon, C. E. (1948). A Mathematical Theory of Communication.
2. Harris, Z. S. (1954). Distributional structure.
3. Mikolov, T., et al. (2013). Efficient Estimation of Word Representations in Vector Space.
4. Bengio, Y., et al. (2003). A Neural Probabilistic Language Model.
