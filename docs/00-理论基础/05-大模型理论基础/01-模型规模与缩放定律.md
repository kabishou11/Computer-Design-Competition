# 第一章：模型规模与缩放定律

## 1.1 缩放定律（Scaling Laws）

### 1.1.1 核心发现

Kaplan et al. (2021) 在论文"Scaling Laws for Neural Language Models"中发现：

> 语言模型的性能（损失）与模型规模、数据规模、计算量之间存在简单的幂律关系

**数学表达**：

$$L(N) = A \cdot N^{-\alpha} + L_\infty$$

$$L(D) = B \cdot D^{-\beta} + L_\infty$$

$$L(C) = C \cdot C^{-\gamma} + L_\infty$$

其中：
- $N$：模型参数量
- $D$：训练数据量
- $C$：计算量（FLOPs）
- $\alpha, \beta, \gamma$：幂律指数
- $L_\infty$：不可降低的损失

### 1.1.2 三个关键定律

**计算量定律**：

$$L(C) \approx C^{-\gamma} \quad \gamma \approx 0.05$$

当计算量增加10倍，损失下降约21%。

**参数量定律**：

$$L(N) \propto N^{-\alpha}$$

固定计算量时，存在最优的参数-数据配比。

**数据定律**：

$$L(D) \propto D^{-\beta}$$

数据量翻倍，损失降低约5%。

### 1.1.3 临界点与涌现能力

**涌现能力的定义**（Wei et al., 2022）：

> 当模型规模超过某个临界点后，突然获得小模型不具备的能力

**典型涌现能力**：

| 能力 | 涌现临界点 |
|-----|-----------|
| 3位数加减法 | ~10B |
| 英文-中文翻译 | ~100B |
| 复杂逻辑推理 | ~100B |
| 少样本学习 | ~10B |

## 1.2 计算最优训练

### 1.2.1 Chinchilla缩放定律

Hoffmann et al. (2022) 提出Chinchilla定律：

**关键发现**：
- Kaplan的结论基于计算量固定时的参数-数据权衡
- 最优配置下，参数量与数据量应成**线性关系**

**Chinchilla配置**：

$$D \approx 20N$$

对于100B参数模型，需要约2T tokens。

### 1.2.2 配置选择指南

| 参数量 | 推荐数据量 | 计算预算 |
|-------|-----------|---------|
| 1B | 20B tokens | ~1e21 FLOPs |
| 7B | 140B tokens | ~1e22 FLOPs |
| 70B | 1.4T tokens | ~1e23 FLOPs |
| 400B | 8T tokens | ~1e24 FLOPs |

### 1.2.3 现实约束

实际训练需要权衡：

1. **显存限制**：参数量的约束
2. **数据可得性**：可用的训练数据
3. **训练时间**：项目周期
4. **成本预算**：云计算费用

## 1.3 模型规模选择

### 1.3.1 场景分析

| 场景 | 推荐规模 | 原因 |
|-----|---------|------|
| 个人学习 | 1.5B-7B | 可本地运行 |
| 研究实验 | 7B-13B | 平衡效果与成本 |
| 竞赛项目 | 7B-70B | 效果优先 |
| 产品部署 | 7B-70B | 效果与成本平衡 |

### 1.3.2 硬件考量

| 规模 | 显存需求 | 推荐硬件 |
|-----|---------|---------|
| 1.5B | 8GB | RTX 3090/4090 |
| 7B | 24-48GB | A100 40GB ×1 |
| 13B | 48-80GB | A100 80GB ×1 |
| 70B | 300-400GB | A100 80GB ×4-8 |

### 1.3.3 开源模型参考

| 模型 | 参数量 | 显存 | 效果评估 |
|-----|-------|------|---------|
| Qwen2.5-1.5B | 1.5B | 8GB | 入门级 |
| Llama3-8B | 8B | 24GB | 良好 |
| Qwen2.5-7B | 7B | 24GB | 良好 |
| Yi-1.5-34B | 34B | 48GB | 优秀 |

## 1.4 资源受限策略

### 1.4.1 量化技术

| 精度 | 显存减少 | 效果影响 |
|-----|---------|---------|
| FP16 | 1× | 基准 |
| INT8 | ~2× | 微小 |
| INT4 | ~4× | 可接受 |
| NF4 | ~4× | 较小 |

### 1.4.2 知识蒸馏

从大模型蒸馏到小模型：

$$\mathcal{L}_{KD} = T^2 \cdot KL(P_{teacher} || P_{student}/T)$$

其中 $T$ 是温度参数。

### 1.4.3 高效架构

| 架构 | 特点 | 适用场景 |
|-----|------|---------|
| MoE | 稀疏激活 | 超大规模 |
| MQA | 共享K/V | 推理加速 |
| GQA | 组共享 | 平衡 |

## 1.5 本章小结

**核心观点**：

1. 缩放定律揭示了规模与性能的幂律关系
2. 存在最优的参数-数据配比（Chinchilla配置）
3. 某些能力在临界点涌现
4. 资源受限时需要权衡选择

**实践建议**：

- 根据可用资源选择合适规模
- 优先保证数据质量
- 利用量化技术扩展能力边界
