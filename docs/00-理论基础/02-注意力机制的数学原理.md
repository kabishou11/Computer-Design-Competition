# 第二章：注意力机制的数学原理

## 2.1 注意力机制的本质

### 2.1.1 从信息瓶颈说起

在理解注意力机制之前，我们需要理解一个核心问题：

**神经网络如何选择性地关注输入的某些部分？**

传统的seq2seq模型将整个输入序列压缩成一个固定维度的向量：

$$h = \text{Encoder}(x_1, x_2, ..., x_n)$$

这存在两个根本问题：
1. **信息瓶颈**：无论输入多长，都压缩成固定维度
2. **平等对待**：无法区分输入中不同部分的重要性

注意力机制的核心思想是**软寻址**（Soft Addressing）：

> 不再将所有信息压缩到一个向量，而是根据当前查询，从记忆中检索相关信息

### 2.1.2 注意力机制的数学定义

给定一组键值对 $(K, V) = \{(k_1, v_1), ..., (k_m, v_m)\}$ 和查询 $q$，注意力机制输出：

$$\text{Attention}(q, K, V) = \sum_{i=1}^{m} \alpha_i v_i$$

其中注意力权重 $\alpha_i$ 由查询和键的相似度计算：

$$\alpha_i = \text{softmax}\left(\frac{q^T k_i}{\sqrt{d_k}}\right)$$

**这个公式的三个核心要素**：

1. **相似度计算** $q^T k_i$：衡量查询和每个键的匹配程度
2. **缩放** $\sqrt{d_k}$：防止点积结果过大导致softmax梯度消失
3. **加权求和**：根据相似度加权组合值

## 2.2 缩放点积注意力

### 2.2.1 公式推导

Transformer使用的缩放点积注意力：

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**为什么是缩放点积？**

假设 $q$ 和 $k$ 是独立同分布的随机向量，每个分量服从均值为0、方差为1的分布。

那么 $q^T k = \sum_{i=1}^{d_k} q_i k_i$ 是 $d_k$ 个独立随机变量的和。

- 每个 $q_i k_i$ 的期望为0，方差为1
- $q^T k$ 的期望为0，方差为 $d_k$

当 $d_k$ 较大时，$q^T k$ 的方差很大，导致softmax进入梯度极小的区域。

**缩放因子 $\sqrt{d_k$** 将方差归一化为1。

### 2.2.2 计算复杂度分析

**标准RNN复杂度**（每个时间步）：

- 时间复杂度：$O(n \cdot d^2)$
- 无法并行

**注意力机制复杂度**：

- 时间复杂度：$O(n^2 \cdot d)$ （Q、K、V均为 $n \times d$ 矩阵）
- 可完全并行化

**空间复杂度**：$O(n^2)$ 用于存储注意力矩阵

### 2.2.3 代码实现（纯数学视角）

```python
import torch
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    缩放点积注意力

    Q: [batch_size, num_heads, seq_len, d_k]
    K: [batch_size, num_heads, seq_len, d_k]
    V: [batch_size, num_heads, seq_len, d_v]
    """
    d_k = Q.size(-1)

    # 1. 计算相似度矩阵
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)

    # 2. 应用mask（如有）
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)

    # 3. Softmax归一化
    attention_weights = F.softmax(scores, dim=-1)

    # 4. 加权求和
    output = torch.matmul(attention_weights, V)

    return output, attention_weights
```

## 2.3 多头注意力

### 2.3.1 为什么需要多头

单头注意力的局限：
- 只能学习一种类型的依赖关系
- 不同头可以学习不同的模式（语法、语义、位置等）

**多头注意力公式**：

$$\text{MultiHead}(Q, K, V) = \text{Concat}(head_1, head_2, ..., head_h)W^O$$

其中每个头：

$$head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

### 2.3.2 参数量分析

对于$d_{model}$维输入，$h$个注意力头：

| 参数 | 形状 | 参数量 |
|-----|------|--------|
| $W_i^Q$ | $[d_{model}, d_k]$ | $h \cdot d_{model} \cdot d_k$ |
| $W_i^K$ | $[d_{model}, d_k]$ | $h \cdot d_{model} \cdot d_k$ |
| $W_i^V$ | $[d_{model}, d_v]$ | $h \cdot d_{model} \cdot d_v$ |
| $W^O$ | $[h \cdot d_v, d_{model}]$ | $h \cdot d_v \cdot d_{model}$ |

**关键设置**：
- $d_k = d_v = d_{model} / h$
- 总参数量与单头注意力相同（只是投影到多个低维空间）

### 2.3.3 不同头学到了什么

研究表明，不同注意力头确实学习到了不同的模式：

**头1-语法头**：关注句法依赖（主语-动词关系）

**头2-语义头**：关注实体关系（谁-做什么）

**头3-位置头**：关注位置邻近

```python
# 多头注意力实现
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, d_k=None, d_v=None):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_k or d_model // num_heads
        self.d_v = d_v or d_model // num_heads

        # 线性投影
        self.W_q = nn.Linear(d_model, self.num_heads * self.d_k)
        self.W_k = nn.Linear(d_model, self.num_heads * self.d_k)
        self.W_v = nn.Linear(d_model, self.num_heads * self.d_v)
        self.W_o = nn.Linear(self.num_heads * self.d_v, d_model)

    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)

        # 1. 线性投影并分头
        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)

        # 2. 计算注意力
        attention_output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)

        # 3. 合并多头
        attention_output = attention_output.transpose(1, 2).contiguous()
        attention_output = attention_output.view(batch_size, -1, self.num_heads * self.d_v)

        # 4. 输出投影
        output = self.W_o(attention_output)

        return output
```

## 2.4 注意力机制的变体

### 2.4.1 掩码注意力（Masked Attention）

用于解码器，防止看到未来位置：

```python
def generate_square_subsequent_mask(size):
    """生成因果掩码"""
    mask = torch.triu(torch.ones(size, size), diagonal=1).bool()
    return mask  # 上三角为True（需要掩蔽）
```

**原理**：位置i只能关注位置≤i的token

### 2.4.2 交叉注意力（Cross Attention）

解码器关注编码器的输出：

$$\text{CrossAttention}(Q_{dec}, K_{enc}, V_{enc}) = \text{softmax}\left(\frac{Q_{dec}K_{enc}^T}{\sqrt{d_k}}\right)V_{enc}$$

**关键区别**：Q来自解码器，K和V来自编码器

### 2.4.3 稀疏注意力（Sparse Attention）

标准注意力的复杂度是 $O(n^2)$，当序列长度增加时不可行。

**稀疏模式**：

1. **局部窗口**：只关注固定窗口内的token
2. **固定步长**：每隔k个token关注一次
3. **全局token**：少数全局token连接到所有位置

**Longformer / BigBird** 使用稀疏注意力的组合。

### 2.4.4 线性注意力（Linear Attention）

将softmax替换为核函数：

$$\text{Attention}(Q, K, V) = \frac{\phi(Q)(\phi(K)^T V)}{\phi(Q)(\phi(K)^T \mathbf{1})}$$

**复杂度**：$O(n \cdot d^2)$ 而非 $O(n^2)$

## 2.5 注意力机制的物理意义

### 2.5.1 软寻址的视角

- **键（Key）**：记忆中的地址
- **值（Value）**：存储的内容
- **查询（Query）**：要查找的内容
- **注意力权重**：软寻址的概率分布

### 2.5.2 能量模型的视角

注意力权重可以看作能量函数：

$$E(q, k_i) = -q^T k_i$$

$$\alpha_i = \frac{\exp(-E(q, k_i)/\tau)}{\sum_j \exp(-E(q, k_j)/\tau)}$$

其中温度 $\tau$ 控制分布的锐利程度。

### 2.5.3 信息流动的视角

注意力机制定义了信息在网络中的流动：

- **编码器自注意力**：输入token之间的信息交互
- **交叉注意力**：编码器到解码器的信息传递
- **解码器自注意力**：输出token之间的信息交互

## 2.6 本章小结

本章深入分析了注意力机制的数学原理：

| 概念 | 公式 | 核心意义 |
|-----|------|---------|
| 缩放点积 | $\frac{QK^T}{\sqrt{d_k}}$ | 相似度计算，防止梯度消失 |
| Softmax归一化 | $\text{softmax}(x)$ | 概率分布化 |
| 多头机制 | $\text{Concat}(heads)W^O$ | 多模式学习 |
| 掩码机制 | $M_{ij}=0$ if $j>i$ | 防止信息泄露 |

**关键洞察**：
1. 注意力是一种"软寻址"机制
2. 缩放因子解决了梯度消失问题
3. 多头让模型学习多种依赖模式

## 参考文献

1. Bahdanau, D., et al. (2014). Neural Machine Translation by Jointly Learning to Align and Translate.
2. Vaswani, A., et al. (2017). Attention Is All You Need.
3. Clark, K., et al. (2019). What Does BERT Look At?
4. Michel, P., et al. (2019). Are Sixteen Heads Really Better than One?
