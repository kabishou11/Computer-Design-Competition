# 第三章：预训练语言模型

## 3.1 预训练与微调范式

### 3.1.1 什么是预训练

预训练（Pre-training）是指在大规模无标注文本数据上训练语言模型，使其学习到通用的语言知识和世界知识。

```python
# 预训练 vs 微调对比
"""
预训练阶段:
- 数据: 海量无标注文本（互联网、书籍、论文等）
- 目标: 学习语言模型 next token prediction
- 成本: 极高（需要大量GPU和时间）

微调阶段:
- 数据: 少量标注数据（任务相关）
- 目标: 适配特定任务
- 成本: 相对较低
"""
```

### 3.1.2 预训练语料

| 数据类型 | 来源 | 特点 |
|---------|------|------|
| 网页数据 | Common Crawl | 规模大，质量参差不齐 |
| 书籍语料 | BookCorpus, 豆瓣读书 | 质量高，叙事性强 |
| 学术论文 | arXiv, PubMed | 专业性强，引用丰富 |
| 代码数据 | GitHub, Stack Overflow | 结构化，逻辑性强 |
| 对话数据 | 社交媒体, 客服记录 | 口语化，多轮交互 |

### 3.1.3 语料预处理

```python
import re
import json
from pathlib import Path

class TextPreprocessor:
    """文本预处理器"""

    def __init__(self, min_length=30, max_length=4096):
        self.min_length = min_length
        self.max_length = max_length

    def clean_text(self, text):
        """清理文本"""
        # 移除HTML标签
        text = re.sub(r'<[^>]+>', '', text)
        # 移除URL
        text = re.sub(r'http[s]?://\S+', '', text)
        # 移除多余空白
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def filter_by_length(self, text):
        """按长度过滤"""
        return self.min_length <= len(text) <= self.max_length

    def process_file(self, input_path, output_path):
        """处理文件"""
        with open(input_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        processed = []
        for line in lines:
            text = self.clean_text(line)
            if self.filter_by_length(text):
                processed.append(json.dumps({'text': text}, ensure_ascii=False))

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(processed))

# 使用示例
preprocessor = TextPreprocessor(min_length=50, max_length=2048)
preprocessor.process_file('raw_data.txt', 'processed_data.jsonl')
```

## 3.2 语言模型的训练目标

### 3.2.1 自回归语言建模（Autoregressive LM）

GPT系列模型采用的训练方式，预测下一个token：

```
P(x) = P(x₁) * P(x₂|x₁) * P(x₃|x₁,x₂) * ... * P(xₙ|x₁,...,xₙ₋₁)
```

```python
import torch
import torch.nn as nn

class AutoregressiveLM(nn.Module):
    def __init__(self, vocab_size, d_model, num_heads, num_layers, max_len):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_len)
        self.transformer = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model, num_heads),
            num_layers
        )
        self.linear = nn.Linear(d_model, vocab_size)

    def forward(self, input_ids, tgt_mask=None):
        embedded = self.pos_encoder(self.embedding(input_ids))
        output = self.transformer(embedded, memory)
        return self.linear(output)

    def generate(self, start_token, max_length, temperature=1.0):
        """自回归生成"""
        generated = [start_token]
        for _ in range(max_length):
            input_ids = torch.tensor([generated])
            logits = self.forward(input_ids)
            probs = torch.softmax(logits[-1] / temperature, dim=-1)
            next_token = torch.multinomial(probs, 1).item()
            generated.append(next_token)
        return generated
```

### 3.2.2 掩码语言建模（Masked LM）

BERT采用的训练方式，预测被掩码的token：

```
输入: 今天天气[MASK]好，我们去[MASK]步
目标: 预测[MASK]位置的词
```

```python
class MaskedLMTrainer:
    """掩码语言模型训练器"""

    def __init__(self, model, tokenizer, mlm_probability=0.15):
        self.model = model
        self.tokenizer = tokenizer
        self.mlm_probability = mlm_probability

    def random_mask_tokens(self, input_ids):
        """随机掩码token"""
        labels = input_ids.clone()
        probability_matrix = torch.full(labels.shape, self.mlm_probability)

        # 特殊token不掩码
        special_tokens_mask = [
            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)
            for val in labels.tolist()
        ]
        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)

        probability_matrix.masked_fill_(special_tokens_mask, 0.0)

        # 80%替换为[MASK]，10%替换为随机token，10%保持不变
        masked_indices = torch.bernoulli(probability_matrix).bool()
        labels[~masked_indices] = -100  # 非掩码位置label设为-100

        # 80% [MASK]
        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices
        input_ids[indices_replaced] = self.tokenizer.mask_token_id

        # 10% 随机token
        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced
        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)
        input_ids[indices_random] = random_words[indices_random]

        return input_ids, labels

    def compute_loss(self, input_ids, attention_mask):
        """计算MLM损失"""
        input_ids, labels = self.random_mask_tokens(input_ids)
        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)

        # 只计算掩码位置的损失
        shift_logits = outputs.logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()

        loss_fct = nn.CrossEntropyLoss()
        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

        return loss
```

## 3.3 三种模型架构对比

| 特性 | Encoder-only | Decoder-only | Encoder-Decoder |
|-----|--------------|--------------|-----------------|
| **注意力** | 双向 | 单向（因果） | 双向 + 交叉 |
| **训练目标** | MLM | CLM | Seq2Seq |
| **适用任务** | 理解为主 | 生成为主 | 理解+生成 |
| **代表模型** | BERT, RoBERTa | GPT, LLaMA | T5, BART |
| **推理效率** | 中等 | 高 | 较低 |
| **上下文利用** | 双向 | 单向 | 双向 |

### 3.3.1 Encoder-only模型（BERT）

```python
from transformers import BertModel, BertTokenizer

model_name = "bert-base-chinese"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# 示例
text = "今天天气真好"
inputs = tokenizer(text, return_tensors="pt")

outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
pooler_output = outputs.pooler_output  # [CLS]位置的输出

print("Last hidden state shape:", last_hidden_state.shape)  # (1, seq_len, 768)
print("Pooler output shape:", pooler_output.shape)  # (1, 768)
```

### 3.3.2 Decoder-only模型（GPT）

```python
from transformers import GPT2Model, GPT2Tokenizer

model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2Model.from_pretrained(model_name)

# GPT-2没有pad token，设置一下
tokenizer.pad_token = tokenizer.eos_token

inputs = tokenizer("今天天气", return_tensors="pt")
outputs = model(**inputs)

last_hidden_state = outputs.last_hidden_state
print("Last hidden state shape:", last_hidden_state.shape)
```

### 3.3.3 Encoder-Decoder模型（T5）

```python
from transformers import T5Model, T5Tokenizer

model_name = "t5-small"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5Model.from_pretrained(model_name)

# T5使用任务前缀
inputs = tokenizer("summarize: 今天天气真好，适合去公园散步。", return_tensors="pt")
outputs = model(input_ids=inputs.input_ids, decoder_input_ids=inputs.input_ids)

print("Output shape:", outputs.last_hidden_state.shape)
```

## 3.4 主流预训练模型介绍

### 3.4.1 开源模型列表

| 模型 | 机构 | 参数规模 | 特点 |
|-----|------|---------|------|
| **LLaMA** | Meta | 7B-70B | 开源先驱，学术友好 |
| **Qwen** | 阿里 | 1.5B-110B | 中文优化好 |
| **Yi** | 零一万物 | 6B-34B | 中英双语强 |
| **ChatGLM** | 智谱 | 6B | 对话优化 |
| **Baichuan** | 百川 | 7B-13B | 中文能力好 |
| **Mistral** | Mistral AI | 7B | 欧洲开源 |

### 3.4.2 模型下载与使用

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# 方法1：在线加载（自动下载）
model_name = "Qwen/Qwen2.5-7B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# 方法2：本地加载
model_path = "./models/Qwen2.5-7B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto"
)

# 推理
prompt = "介绍一下人工智能的发展历史。"
messages = [{"role": "user", "content": prompt}]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(text, return_tensors="pt").to(model.device)

outputs = model.generate(**inputs, max_new_tokens=512)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## 3.5 训练策略

### 3.5.1 混合精度训练

```python
from accelerate import Accelerator

# 使用Accelerator实现混合精度训练
accelerator = Accelerator(mixed_precision="fp16")

model, optimizer, training_dataloader = accelerator.prepare(
    model, optimizer, training_dataloader
)

for batch in training_dataloader:
    with accelerator.autocast():
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
```

### 3.5.2 梯度累积

```python
# 梯度累积：模拟更大的batch size
accumulation_steps = 8

for epoch in range(num_epochs):
    for step, batch in enumerate(dataloader):
        outputs = model(**batch)
        loss = outputs.loss / accumulation_steps
        loss.backward()

        if (step + 1) % accumulation_steps == 0:
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
```

## 3.6 本章小结

本章介绍了预训练语言模型的核心知识：
- 预训练语料的收集与预处理
- 自回归和掩码两种训练目标
- Encoder、Decoder、Encoder-Decoder三种架构
- 主流预训练模型介绍
- 训练优化策略

## 练习题

1. 收集并预处理一个领域特定的语料库
2. 比较BERT和GPT的预训练目标差异
3. 在本地加载一个开源大模型并进行推理
4. 实现混合精度训练代码

## 参考资料

- [BERT Paper](https://arxiv.org/abs/1810.04805) - BERT原始论文
- [GPT Paper](https://arxiv.org/abs/2005.14165) - GPT-3论文
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) - GPT-3
- [Hugging Face Transformers](https://huggingface.co/docs/transformers) - 官方文档
