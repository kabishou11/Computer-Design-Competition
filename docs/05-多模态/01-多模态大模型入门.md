# 多模态大模型入门

## 1. 多模态概述

### 1.1 什么是多模态

多模态（Multimodal）指同时处理多种类型数据的能力：

| 模态 | 数据类型 | 示例 |
|-----|---------|------|
| **视觉** | 图像、视频 | 图片理解、视频分析 |
| **听觉** | 语音、音频 | 语音识别、音乐生成 |
| **文本** | 自然语言 | 文本理解、生成 |
| **传感器** | 点云、深度 | 3D感知 |

### 1.2 多模态模型类型

```
多模态模型分类

├── 对齐型 (Alignment)
│   └── 将不同模态映射到统一空间
│       CLIP, BLIP
│
├── 生成型 (Generation)
│   ├── 图像生成：Stable Diffusion, DALL-E
│   └── 视频生成：Sora, VideoDiT
│
├── 理解型 (Understanding)
│   ├── 视觉问答：LLaVA, MiniGPT-4
│   └── 图文理解：Flamingo, Kosmos
│
└── 统一型 (Unified)
    ├── 多模态输入：任意组合
    └── 多模态输出：文本+图像
```

## 2. 视觉语言模型

### 2.1 LLaVA实战

```python
from llava.model.builder import load_pretrained_model
from llava.mm_utils import get_model_name_from_path
from llava.eval.run_llava import eval_model

class LLaVAInference:
    """LLaVA视觉语言模型推理"""

    def __init__(self, model_path="liuhaotian/LLaVA-Lightning-7B Vicuna"):
        model_name = get_model_name_from_path(model_path)
        self.tokenizer, self.model, self.image_processor, self.context_len = \
            load_pretrained_model(model_path, None, model_name)

    def inference(self, image_path: str, prompt: str) -> str:
        """图像问答"""
        args = type('Args', (), {
            "image_file": image_path,
            "text_input": prompt,
            "model_path": "liuhaotian/LLaVA-Lightning-7B Vicuna",
            "model_name": "llava-v1.5-7b",
            "conv_mode": None,
            "temperature": 0,
            "top_p": None,
            "num_beams": 1
        })()

        return eval_model(args)
```

### 2.2 BLIP-2实战

```python
from transformers import AutoProcessor, Blip2ForConditionalGeneration
from PIL import Image

class BLIP2Inference:
    """BLIP-2视觉语言模型"""

    def __init__(self, model_name="Salesforce/blip2-flan-t5-xxl"):
        self.processor = AutoProcessor.from_pretrained(model_name)
        self.model = Blip2ForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype="auto"
        )

    def image_to_text(self, image: Image.Image, prompt: str = None) -> str:
        """图像理解"""
        if prompt:
            inputs = self.processor(
                image,
                text=prompt,
                return_tensors="pt"
            )
        else:
            inputs = self.processor(image, return_tensors="pt")

        outputs = self.model.generate(**inputs, max_new_tokens=300)
        return self.processor.decode(outputs[0], skip_special_tokens=True)

    def visual_qa(self, image: Image.Image, question: str) -> str:
        """视觉问答"""
        prompt = f"Question: {question} Answer:"
        return self.image_to_text(image, prompt)

    def image_captioning(self, image: Image.Image) -> str:
        """图像描述"""
        return self.image_to_text(image, None)
```

### 2.3 商用API调用

```python
import base64
import requests

class MultimodalAPI:
    """多模态API调用"""

    # 通义千问VL
    def qwen_vl_inference(self, image_path: str, prompt: str) -> str:
        """调用通义千问视觉模型"""
        with open(image_path, "rb") as f:
            image_base64 = base64.b64encode(f.read()).decode()

        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {API_KEY}"}
        payload = {
            "model": "qwen-vl-max",
            "messages": [{
                "role": "user",
                "content": [
                    {"type": "image", "image": f"data:image/jpeg;base64,{image_base64}"},
                    {"type": "text", "text": prompt}
                ]
            }]
        }

        response = requests.post(API_URL, headers=headers, json=payload)
        return response.json()["choices"][0]["message"]["content"]

    # 智谱GLM-4V
    def glm4v_inference(self, image_path: str, prompt: str) -> str:
        """调用智谱GLM-4V模型"""
        # 类似实现
        pass

    # Claude 3 Vision
    def claude_vision_inference(self, image_path: str, prompt: str) -> str:
        """调用Claude 3视觉能力"""
        with open(image_path, "rb") as f:
            image_data = base64.b64encode(f.read()).decode()

        headers = {
            "x-api-key": API_KEY,
            "Content-Type": "application/json"
        }
        payload = {
            "model": "claude-3-sonnet-20240229",
            "max_tokens": 1024,
            "messages": [{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image", "source": {"type": "base64", "media_type": "image/jpeg", "data": image_data}}
                ]
            }]
        }

        response = requests.post(API_URL, headers=headers, json=payload)
        return response.json()["content"][0]["text"]
```

## 3. 语音处理

### 3.1 语音识别 (ASR)

```python
from funasr import AutoModel

class SpeechRecognition:
    """语音识别"""

    def __init__(self, model="paraformer-zh"):
        self.model = AutoModel(
            model=model,
            vad_model="fsmn-vad",
            punc_model="ct-punc",
            spk_model=" camsim"
        )

    def recognize(self, audio_path: str) -> str:
        """识别语音"""
        return self.model.generate(input=audio_path)[0]["text"]

    def recognize_streaming(self, audio_stream):
        """流式识别"""
        for result in self.model.generate(
            input=audio_stream,
            cache={},
            language="auto",
            use_itn=True
        ):
            yield result["text"]
```

### 3.2 语音合成 (TTS)

```python
import torch
from modelscope import AutoModelForTextToSpeech

class TextToSpeech:
    """语音合成"""

    def __init__(self, model="speech_tts"):
        self.model = AutoModelForTextToSpeech.from_pretrained(model)

    def synthesize(self, text: str, output_path: str):
        """合成语音"""
        import soundfile as sf

        # 生成
        output = self.model.generate(
            input=text,
            speaker_id=0
        )

        # 保存
        sf.write(output_path, output["audio_data"].numpy(), output["sample_rate"])
```

## 4. 多模态应用场景

### 4.1 视觉文档理解

```python
class DocumentUnderstanding:
    """文档理解系统"""

    def __init__(self):
        self.ocr = OCRModel()
        self.vlm = LLaVAInference()
        self.structure_parser = StructureParser()

    def analyze_document(self, image_path: str) -> Dict:
        """分析文档"""
        # 1. OCR提取文本
        ocr_result = self.ocr.extract(image_path)

        # 2. 识别文档结构
        structure = self.structure_parser.parse(image_path)

        # 3. 视觉问答理解内容
        questions = [
            "这是什么类型的文档？",
            "文档的主要标题是什么？",
            "提取关键信息：日期、金额、人物"
        ]

        vqa_results = []
        for q in questions:
            answer = self.vlm.inference(image_path, q)
            vqa_results.append({"question": q, "answer": answer})

        return {
            "ocr_text": ocr_result,
            "structure": structure,
            "vqa_analysis": vqa_results
        }
```

### 4.2 多模态客服

```python
class MultimodalCustomerService:
    """多模态客服系统"""

    def __init__(self):
        self.vision_model = LLaVAInference()
        self.text_llm = LocalLLM()
        self.knowledge_base = VectorStore()

    def handle_request(
        self,
        text: str = None,
        image: Image.Image = None
    ) -> str:
        """处理用户请求"""
        context = []

        # 处理文本
        if text:
            context.append(f"用户文本：{text}")

        # 处理图像
        if image:
            # 图像理解
            image_description = self.vision_model.inference(
                image,
                "请详细描述这张图片的内容"
            )
            context.append(f"用户图片描述：{image_description}")

        # 检索知识库
        knowledge = self.knowledge_base.search("\n".join(context))

        # 生成回答
        prompt = f"""
用户输入：
{chr(10).join(context)}

相关知识：
{chr(10).join([k[0] for k in knowledge])}

请生成回答：
"""
        return self.text_llm.generate(prompt)
```

## 5. 多模态发展趋势

### 5.1 2024-2025趋势

| 趋势 | 说明 | 代表工作 |
|-----|------|---------|
| **多模态统一** | 单一模型处理所有模态 | GPT-4o, Gemini |
| **端到端** | 从输入直接到输出 | VILA, LLaVA-NeXT |
| **视频理解** | 长视频理解 | Video-LLaMA |
| **3D理解** | 点云、场景理解 | Point-BERT |

### 5.2 竞赛应用建议

对于计算机设计大赛，多模态方向推荐：

| 项目类型 | 技术要点 | 难度 | 推荐度 |
|---------|---------|------|-------|
| **文档智能** | OCR + VLM + 信息提取 | ⭐⭐⭐ | 高 |
| **视觉问答** | VLM微调 | ⭐⭐ | 高 |
| **图像生成** | Stable Diffusion | ⭐⭐⭐ | 中 |
| **多模态搜索** | CLIP + 向量检索 | ⭐⭐ | 高 |
| **视频理解** | 视频VLM | ⭐⭐⭐⭐ | 低 |

## 6. 本章小结

本章介绍了多模态大模型：
- 多模态模型概述和分类
- 视觉语言模型（LLaVA, BLIP-2）
- 语音识别和合成
- 多模态应用场景
- 发展趋势和竞赛建议

## 参考资料

- [LLaVA](https://llava-vl.github.io/) - 视觉指令微调
- [BLIP-2](https://huggingface.co/docs/transformers/main/en/model_doc/blip-2) - BLIP-2文档
- [Open Multimodal](https://github.com/Computer-Vision-Team/multimodal) - 多模态工具集
